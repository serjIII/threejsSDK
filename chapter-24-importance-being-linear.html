<!doctype html><html lang="en" dir="ltr" xmlns:og="http://ogp.me/ns#"><head profile="http://www.w3.org/1999/xhtml/vocab"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta charset="utf-8"><script type="text/javascript">(window.NREUM||(NREUM={})).init={ajax:{deny_list:["bam.nr-data.net"]}};(window.NREUM||(NREUM={})).loader_config={licenseKey:"6f2048d7bc",applicationID:"341156206"};;/*! For license information please see nr-loader-rum-1.236.0.min.js.LICENSE.txt */
(()=>{"use strict";var e,t,n={5763:(e,t,n)=>{n.d(t,{P_:()=>f,Mt:()=>p,C5:()=>s,DL:()=>m,OP:()=>_,lF:()=>E,Yu:()=>y,Dg:()=>g,CX:()=>c,GE:()=>b,sU:()=>j});var r=n(8632),i=n(9567);const o={beacon:r.ce.beacon,errorBeacon:r.ce.errorBeacon,licenseKey:void 0,applicationID:void 0,sa:void 0,queueTime:void 0,applicationTime:void 0,ttGuid:void 0,user:void 0,account:void 0,product:void 0,extra:void 0,jsAttributes:{},userAttributes:void 0,atts:void 0,transactionName:void 0,tNamePlain:void 0},a={};function s(e){if(!e)throw new Error("All info objects require an agent identifier!");if(!a[e])throw new Error("Info for ".concat(e," was never set"));return a[e]}function c(e,t){if(!e)throw new Error("All info objects require an agent identifier!");a[e]=(0,i.D)(t,o),(0,r.Qy)(e,a[e],"info")}var d=n(7056);const u=()=>{const e={blockSelector:"[data-nr-block]",maskInputOptions:{password:!0}};return{allow_bfcache:!0,privacy:{cookies_enabled:!0},ajax:{deny_list:void 0,enabled:!0,harvestTimeSeconds:10},distributed_tracing:{enabled:void 0,exclude_newrelic_header:void 0,cors_use_newrelic_header:void 0,cors_use_tracecontext_headers:void 0,allowed_origins:void 0},session:{domain:void 0,expiresMs:d.oD,inactiveMs:d.Hb},ssl:void 0,obfuscate:void 0,jserrors:{enabled:!0,harvestTimeSeconds:10},metrics:{enabled:!0},page_action:{enabled:!0,harvestTimeSeconds:30},page_view_event:{enabled:!0},page_view_timing:{enabled:!0,harvestTimeSeconds:30,long_task:!1},session_trace:{enabled:!0,harvestTimeSeconds:10},harvest:{tooManyRequestsDelay:60},session_replay:{enabled:!1,harvestTimeSeconds:60,sampleRate:.1,errorSampleRate:.1,maskTextSelector:"*",maskAllInputs:!0,get blockClass(){return"nr-block"},get ignoreClass(){return"nr-ignore"},get maskTextClass(){return"nr-mask"},get blockSelector(){return e.blockSelector},set blockSelector(t){e.blockSelector+=",".concat(t)},get maskInputOptions(){return e.maskInputOptions},set maskInputOptions(t){e.maskInputOptions={...t,password:!0}}},spa:{enabled:!0,harvestTimeSeconds:10}}},l={};function f(e){if(!e)throw new Error("All configuration objects require an agent identifier!");if(!l[e])throw new Error("Configuration for ".concat(e," was never set"));return l[e]}function g(e,t){if(!e)throw new Error("All configuration objects require an agent identifier!");l[e]=(0,i.D)(t,u()),(0,r.Qy)(e,l[e],"config")}function p(e,t){if(!e)throw new Error("All configuration objects require an agent identifier!");var n=f(e);if(n){for(var r=t.split("."),i=0;i<r.length-1;i++)if("object"!=typeof(n=n[r[i]]))return;n=n[r[r.length-1]]}return n}const h={accountID:void 0,trustKey:void 0,agentID:void 0,licenseKey:void 0,applicationID:void 0,xpid:void 0},v={};function m(e){if(!e)throw new Error("All loader-config objects require an agent identifier!");if(!v[e])throw new Error("LoaderConfig for ".concat(e," was never set"));return v[e]}function b(e,t){if(!e)throw new Error("All loader-config objects require an agent identifier!");v[e]=(0,i.D)(t,h),(0,r.Qy)(e,v[e],"loader_config")}const y=(0,r.mF)().o;var w=n(385),A=n(6818);const x={buildEnv:A.Re,bytesSent:{},queryBytesSent:{},customTransaction:void 0,disabled:!1,distMethod:A.gF,isolatedBacklog:!1,loaderType:void 0,maxBytes:3e4,offset:Math.floor(w._A?.performance?.timeOrigin||w._A?.performance?.timing?.navigationStart||Date.now()),onerror:void 0,origin:""+w._A.location,ptid:void 0,releaseIds:{},session:void 0,xhrWrappable:"function"==typeof w._A.XMLHttpRequest?.prototype?.addEventListener,version:A.q4},D={};function _(e){if(!e)throw new Error("All runtime objects require an agent identifier!");if(!D[e])throw new Error("Runtime for ".concat(e," was never set"));return D[e]}function j(e,t){if(!e)throw new Error("All runtime objects require an agent identifier!");D[e]=(0,i.D)(t,x),(0,r.Qy)(e,D[e],"runtime")}function E(e){return function(e){try{const t=s(e);return!!t.licenseKey&&!!t.errorBeacon&&!!t.applicationID}catch(e){return!1}}(e)}},9567:(e,t,n)=>{n.d(t,{D:()=>i});var r=n(50);function i(e,t){try{if(!e||"object"!=typeof e)return(0,r.Z)("Setting a Configurable requires an object as input");if(!t||"object"!=typeof t)return(0,r.Z)("Setting a Configurable requires a model to set its initial properties");const n=Object.create(Object.getPrototypeOf(t),Object.getOwnPropertyDescriptors(t)),o=0===Object.keys(n).length?e:n;for(let a in o)if(void 0!==e[a])try{"object"==typeof e[a]&&"object"==typeof t[a]?n[a]=i(e[a],t[a]):n[a]=e[a]}catch(e){(0,r.Z)("An error occurred while setting a property of a Configurable",e)}return n}catch(e){(0,r.Z)("An error occured while setting a Configurable",e)}}},6818:(e,t,n)=>{n.d(t,{Re:()=>i,gF:()=>o,q4:()=>r});const r="1.236.0",i="PROD",o="CDN"},385:(e,t,n)=>{n.d(t,{FN:()=>a,IF:()=>d,Nk:()=>l,Tt:()=>s,_A:()=>o,il:()=>r,pL:()=>c,v6:()=>i,w1:()=>u});const r="undefined"!=typeof window&&!!window.document,i="undefined"!=typeof WorkerGlobalScope&&("undefined"!=typeof self&&self instanceof WorkerGlobalScope&&self.navigator instanceof WorkerNavigator||"undefined"!=typeof globalThis&&globalThis instanceof WorkerGlobalScope&&globalThis.navigator instanceof WorkerNavigator),o=r?window:"undefined"!=typeof WorkerGlobalScope&&("undefined"!=typeof self&&self instanceof WorkerGlobalScope&&self||"undefined"!=typeof globalThis&&globalThis instanceof WorkerGlobalScope&&globalThis),a=""+o?.location,s=/iPad|iPhone|iPod/.test(navigator.userAgent),c=s&&"undefined"==typeof SharedWorker,d=(()=>{const e=navigator.userAgent.match(/Firefox[/\s](\d+\.\d+)/);return Array.isArray(e)&&e.length>=2?+e[1]:0})(),u=Boolean(r&&window.document.documentMode),l=!!navigator.sendBeacon},1117:(e,t,n)=>{n.d(t,{w:()=>o});var r=n(50);const i={agentIdentifier:"",ee:void 0};class o{constructor(e){try{if("object"!=typeof e)return(0,r.Z)("shared context requires an object as input");this.sharedContext={},Object.assign(this.sharedContext,i),Object.entries(e).forEach((e=>{let[t,n]=e;Object.keys(i).includes(t)&&(this.sharedContext[t]=n)}))}catch(e){(0,r.Z)("An error occured while setting SharedContext",e)}}}},8e3:(e,t,n)=>{n.d(t,{L:()=>u,R:()=>c});var r=n(2177),i=n(1284),o=n(4322),a=n(3325);const s={};function c(e,t){const n={staged:!1,priority:a.p[t]||0};d(e),s[e].get(t)||s[e].set(t,n)}function d(e){e&&(s[e]||(s[e]=new Map))}function u(){let e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:"",t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:"feature";if(d(e),!e||!s[e].get(t))return a(t);s[e].get(t).staged=!0;const n=[...s[e]];function a(t){const n=e?r.ee.get(e):r.ee,a=o.X.handlers;if(n.backlog&&a){var s=n.backlog[t],c=a[t];if(c){for(var d=0;s&&d<s.length;++d)l(s[d],c);(0,i.D)(c,(function(e,t){(0,i.D)(t,(function(t,n){n[0].on(e,n[1])}))}))}delete a[t],n.backlog[t]=null,n.emit("drain-"+t,[])}}n.every((e=>{let[t,n]=e;return n.staged}))&&(n.sort(((e,t)=>e[1].priority-t[1].priority)),n.forEach((e=>{let[t]=e;a(t)})))}function l(e,t){var n=e[1];(0,i.D)(t[n],(function(t,n){var r=e[0];if(n[0]===r){var i=n[1],o=e[3],a=e[2];i.apply(o,a)}}))}},2177:(e,t,n)=>{n.d(t,{ee:()=>d});var r=n(8632),i=n(2210),o=n(1284),a=n(5763),s="nr@context";let c=(0,r.fP)();var d;function u(){}function l(){return new u}function f(){d.aborted=!0,d.backlog={}}c.ee?d=c.ee:(d=function e(t,n){var r={},c={},g={},p=!1;try{p=16===n.length&&(0,a.OP)(n).isolatedBacklog}catch(e){}var h={on:b,addEventListener:b,removeEventListener:y,emit:m,get:A,listeners:w,context:v,buffer:x,abort:f,aborted:!1,isBuffering:D,debugId:n,backlog:p?{}:t&&"object"==typeof t.backlog?t.backlog:{}};return h;function v(e){return e&&e instanceof u?e:e?(0,i.X)(e,s,l):l()}function m(e,n,r,i,o){if(!1!==o&&(o=!0),!d.aborted||i){t&&o&&t.emit(e,n,r);for(var a=v(r),s=w(e),u=s.length,l=0;l<u;l++)s[l].apply(a,n);var f=_()[c[e]];return f&&f.push([h,e,n,a]),a}}function b(e,t){r[e]=w(e).concat(t)}function y(e,t){var n=r[e];if(n)for(var i=0;i<n.length;i++)n[i]===t&&n.splice(i,1)}function w(e){return r[e]||[]}function A(t){return g[t]=g[t]||e(h,t)}function x(e,t){var n=_();h.aborted||(0,o.D)(e,(function(e,r){t=t||"feature",c[r]=t,t in n||(n[t]=[])}))}function D(e){return!!_()[c[e]]}function _(){return h.backlog}}(void 0,"globalEE"),c.ee=d)},5546:(e,t,n)=>{n.d(t,{E:()=>r,p:()=>i});var r=n(2177).ee.get("handle");function i(e,t,n,i,o){o?(o.buffer([e],i),o.emit(e,t,n)):(r.buffer([e],i),r.emit(e,t,n))}},4322:(e,t,n)=>{n.d(t,{X:()=>o});var r=n(5546);o.on=a;var i=o.handlers={};function o(e,t,n,o){a(o||r.E,i,e,t,n)}function a(e,t,n,i,o){o||(o="feature"),e||(e=r.E);var a=t[o]=t[o]||{};(a[n]=a[n]||[]).push([e,i])}},3239:(e,t,n)=>{n.d(t,{bP:()=>s,iz:()=>c,m$:()=>a});var r=n(385);let i=!1,o=!1;try{const e={get passive(){return i=!0,!1},get signal(){return o=!0,!1}};r._A.addEventListener("test",null,e),r._A.removeEventListener("test",null,e)}catch(e){}function a(e,t){return i||o?{capture:!!e,passive:i,signal:t}:!!e}function s(e,t){let n=arguments.length>2&&void 0!==arguments[2]&&arguments[2],r=arguments.length>3?arguments[3]:void 0;window.addEventListener(e,t,a(n,r))}function c(e,t){let n=arguments.length>2&&void 0!==arguments[2]&&arguments[2],r=arguments.length>3?arguments[3]:void 0;document.addEventListener(e,t,a(n,r))}},4402:(e,t,n)=>{n.d(t,{Rl:()=>a,ky:()=>s});var r=n(385);const i="xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx";function o(e,t){return e?15&e[t]:16*Math.random()|0}function a(){const e=r._A?.crypto||r._A?.msCrypto;let t,n=0;return e&&e.getRandomValues&&(t=e.getRandomValues(new Uint8Array(31))),i.split("").map((e=>"x"===e?o(t,++n).toString(16):"y"===e?(3&o()|8).toString(16):e)).join("")}function s(e){const t=r._A?.crypto||r._A?.msCrypto;let n,i=0;t&&t.getRandomValues&&(n=t.getRandomValues(new Uint8Array(31)));const a=[];for(var s=0;s<e;s++)a.push(o(n,++i).toString(16));return a.join("")}},7056:(e,t,n)=>{n.d(t,{Bq:()=>r,Hb:()=>o,oD:()=>i});const r="NRBA",i=144e5,o=18e5},7894:(e,t,n)=>{function r(){return Math.round(performance.now())}n.d(t,{z:()=>r})},50:(e,t,n)=>{function r(e,t){"function"==typeof console.warn&&(console.warn("New Relic: ".concat(e)),t&&console.warn(t))}n.d(t,{Z:()=>r})},2587:(e,t,n)=>{n.d(t,{N:()=>c,T:()=>d});var r=n(2177),i=n(5546),o=n(8e3),a=n(3325);const s={stn:[a.D.sessionTrace],err:[a.D.jserrors,a.D.metrics],ins:[a.D.pageAction],spa:[a.D.spa],sr:[a.D.sessionReplay,a.D.sessionTrace]};function c(e,t){const n=r.ee.get(t);e&&"object"==typeof e&&(Object.entries(e).forEach((e=>{let[t,r]=e;void 0===d[t]&&(s[t]?s[t].forEach((e=>{r?(0,i.p)("feat-"+t,[],void 0,e,n):(0,i.p)("block-"+t,[],void 0,e,n),(0,i.p)("rumresp-"+t,[Boolean(r)],void 0,e,n)})):r&&(0,i.p)("feat-"+t,[],void 0,void 0,n),d[t]=Boolean(r))})),Object.keys(s).forEach((e=>{void 0===d[e]&&(s[e]?.forEach((t=>(0,i.p)("rumresp-"+e,[!1],void 0,t,n))),d[e]=!1)})),(0,o.L)(t,a.D.pageViewEvent))}const d={}},2210:(e,t,n)=>{n.d(t,{X:()=>i});var r=Object.prototype.hasOwnProperty;function i(e,t,n){if(r.call(e,t))return e[t];var i=n();if(Object.defineProperty&&Object.keys)try{return Object.defineProperty(e,t,{value:i,writable:!0,enumerable:!1}),i}catch(e){}return e[t]=i,i}},1284:(e,t,n)=>{n.d(t,{D:()=>r});const r=(e,t)=>Object.entries(e||{}).map((e=>{let[n,r]=e;return t(n,r)}))},4351:(e,t,n)=>{n.d(t,{P:()=>o});var r=n(2177);const i=()=>{const e=new WeakSet;return(t,n)=>{if("object"==typeof n&&null!==n){if(e.has(n))return;e.add(n)}return n}};function o(e){try{return JSON.stringify(e,i())}catch(e){try{r.ee.emit("internal-error",[e])}catch(e){}}}},3960:(e,t,n)=>{n.d(t,{K:()=>a,b:()=>o});var r=n(3239);function i(){return"undefined"==typeof document||"complete"===document.readyState}function o(e,t){if(i())return e();(0,r.bP)("load",e,t)}function a(e){if(i())return e();(0,r.iz)("DOMContentLoaded",e)}},8632:(e,t,n)=>{n.d(t,{EZ:()=>d,Qy:()=>c,ce:()=>o,fP:()=>a,gG:()=>u,mF:()=>s});var r=n(7894),i=n(385);const o={beacon:"bam.nr-data.net",errorBeacon:"bam.nr-data.net"};function a(){return i._A.NREUM||(i._A.NREUM={}),void 0===i._A.newrelic&&(i._A.newrelic=i._A.NREUM),i._A.NREUM}function s(){let e=a();return e.o||(e.o={ST:i._A.setTimeout,SI:i._A.setImmediate,CT:i._A.clearTimeout,XHR:i._A.XMLHttpRequest,REQ:i._A.Request,EV:i._A.Event,PR:i._A.Promise,MO:i._A.MutationObserver,FETCH:i._A.fetch}),e}function c(e,t,n){let i=a();const o=i.initializedAgents||{},s=o[e]||{};return Object.keys(s).length||(s.initializedAt={ms:(0,r.z)(),date:new Date}),i.initializedAgents={...o,[e]:{...s,[n]:t}},i}function d(e,t){a()[e]=t}function u(){return function(){let e=a();const t=e.info||{};e.info={beacon:o.beacon,errorBeacon:o.errorBeacon,...t}}(),function(){let e=a();const t=e.init||{};e.init={...t}}(),s(),function(){let e=a();const t=e.loader_config||{};e.loader_config={...t}}(),a()}},7956:(e,t,n)=>{n.d(t,{N:()=>i});var r=n(3239);function i(e){let t=arguments.length>1&&void 0!==arguments[1]&&arguments[1],n=arguments.length>2?arguments[2]:void 0,i=arguments.length>3?arguments[3]:void 0;return void(0,r.iz)("visibilitychange",(function(){if(t)return void("hidden"==document.visibilityState&&e());e(document.visibilityState)}),n,i)}},3081:(e,t,n)=>{n.d(t,{gF:()=>o,mY:()=>i,t9:()=>r,vz:()=>s,xS:()=>a});const r=n(3325).D.metrics,i="sm",o="cm",a="storeSupportabilityMetrics",s="storeEventMetrics"},7633:(e,t,n)=>{n.d(t,{Dz:()=>i,OJ:()=>a,qw:()=>o,t9:()=>r});const r=n(3325).D.pageViewEvent,i="firstbyte",o="domcontent",a="windowload"},9251:(e,t,n)=>{n.d(t,{t:()=>r});const r=n(3325).D.pageViewTiming},5938:(e,t,n)=>{n.d(t,{W:()=>o});var r=n(5763),i=n(2177);class o{constructor(e,t,n){this.agentIdentifier=e,this.aggregator=t,this.ee=i.ee.get(e,(0,r.OP)(this.agentIdentifier).isolatedBacklog),this.featureName=n,this.blocked=!1}}},9144:(e,t,n)=>{n.d(t,{j:()=>v});var r=n(3325),i=n(5763),o=n(5546),a=n(2177),s=n(7894),c=n(8e3),d=n(3960),u=n(385),l=n(50),f=n(3081),g=n(8632);function p(){const e=(0,g.gG)();["setErrorHandler","finished","addToTrace","inlineHit","addRelease","addPageAction","setCurrentRouteName","setPageViewName","setCustomAttribute","interaction","noticeError","setUserId"].forEach((t=>{e[t]=function(){for(var n=arguments.length,r=new Array(n),i=0;i<n;i++)r[i]=arguments[i];return function(t){for(var n=arguments.length,r=new Array(n>1?n-1:0),i=1;i<n;i++)r[i-1]=arguments[i];let o=[];return Object.values(e.initializedAgents).forEach((e=>{e.exposed&&e.api[t]&&o.push(e.api[t](...r))})),o.length>1?o:o[0]}(t,...r)}}))}var h=n(2587);function v(e){let t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{},v=arguments.length>2?arguments[2]:void 0,m=arguments.length>3?arguments[3]:void 0,{init:b,info:y,loader_config:w,runtime:A={loaderType:v},exposed:x=!0}=t;const D=(0,g.gG)();y||(b=D.init,y=D.info,w=D.loader_config),(0,i.Dg)(e,b||{}),(0,i.GE)(e,w||{}),(0,i.sU)(e,A),y.jsAttributes??={},u.v6&&(y.jsAttributes.isWorker=!0),(0,i.CX)(e,y),p();const _=function(e,t){t||(0,c.R)(e,"api");const g={};var p=a.ee.get(e),h=p.get("tracer"),v="api-",m=v+"ixn-";function b(t,n,r,o){const a=(0,i.C5)(e);return null===n?delete a.jsAttributes[t]:(0,i.CX)(e,{...a,jsAttributes:{...a.jsAttributes,[t]:n}}),A(v,r,!0,o||null===n?"session":void 0)(t,n)}function y(){}["setErrorHandler","finished","addToTrace","inlineHit","addRelease"].forEach((e=>g[e]=A(v,e,!0,"api"))),g.addPageAction=A(v,"addPageAction",!0,r.D.pageAction),g.setCurrentRouteName=A(v,"routeName",!0,r.D.spa),g.setPageViewName=function(t,n){if("string"==typeof t)return"/"!==t.charAt(0)&&(t="/"+t),(0,i.OP)(e).customTransaction=(n||"http://custom.transaction")+t,A(v,"setPageViewName",!0)()},g.setCustomAttribute=function(e,t){let n=arguments.length>2&&void 0!==arguments[2]&&arguments[2];if("string"==typeof e){if(["string","number"].includes(typeof t)||null===t)return b(e,t,"setCustomAttribute",n);(0,l.Z)("Failed to execute setCustomAttribute.\nNon-null value must be a string or number type, but a type of <".concat(typeof t,"> was provided."))}else(0,l.Z)("Failed to execute setCustomAttribute.\nName must be a string type, but a type of <".concat(typeof e,"> was provided."))},g.setUserId=function(e){if("string"==typeof e||null===e)return b("enduser.id",e,"setUserId",!0);(0,l.Z)("Failed to execute setUserId.\nNon-null value must be a string type, but a type of <".concat(typeof e,"> was provided."))},g.interaction=function(){return(new y).get()};var w=y.prototype={createTracer:function(e,t){var n={},i=this,a="function"==typeof t;return(0,o.p)(m+"tracer",[(0,s.z)(),e,n],i,r.D.spa,p),function(){if(h.emit((a?"":"no-")+"fn-start",[(0,s.z)(),i,a],n),a)try{return t.apply(this,arguments)}catch(e){throw h.emit("fn-err",[arguments,this,"string"==typeof e?new Error(e):e],n),e}finally{h.emit("fn-end",[(0,s.z)()],n)}}}};function A(e,t,n,i){return function(){return(0,o.p)(f.xS,["API/"+t+"/called"],void 0,r.D.metrics,p),i&&(0,o.p)(e+t,[(0,s.z)(),...arguments],n?null:this,i,p),n?void 0:this}}function x(){n.e(439).then(n.bind(n,7438)).then((t=>{let{setAPI:n}=t;n(e),(0,c.L)(e,"api")})).catch((()=>(0,l.Z)("Downloading runtime APIs failed...")))}return["actionText","setName","setAttribute","save","ignore","onEnd","getContext","end","get"].forEach((e=>{w[e]=A(m,e,void 0,r.D.spa)})),g.noticeError=function(e,t){"string"==typeof e&&(e=new Error(e)),(0,o.p)(f.xS,["API/noticeError/called"],void 0,r.D.metrics,p),(0,o.p)("err",[e,(0,s.z)(),!1,t],void 0,r.D.jserrors,p)},u.il?(0,d.b)((()=>x()),!0):x(),g}(e,m);return(0,g.Qy)(e,_,"api"),(0,g.Qy)(e,x,"exposed"),(0,g.EZ)("activatedFeatures",h.T),_}},3325:(e,t,n)=>{n.d(t,{D:()=>r,p:()=>i});const r={ajax:"ajax",jserrors:"jserrors",metrics:"metrics",pageAction:"page_action",pageViewEvent:"page_view_event",pageViewTiming:"page_view_timing",sessionReplay:"session_replay",sessionTrace:"session_trace",spa:"spa"},i={[r.pageViewEvent]:1,[r.pageViewTiming]:2,[r.metrics]:3,[r.jserrors]:4,[r.ajax]:5,[r.sessionTrace]:6,[r.pageAction]:7,[r.spa]:8,[r.sessionReplay]:9}}},r={};function i(e){var t=r[e];if(void 0!==t)return t.exports;var o=r[e]={exports:{}};return n[e](o,o.exports,i),o.exports}i.m=n,i.d=(e,t)=>{for(var n in t)i.o(t,n)&&!i.o(e,n)&&Object.defineProperty(e,n,{enumerable:!0,get:t[n]})},i.f={},i.e=e=>Promise.all(Object.keys(i.f).reduce(((t,n)=>(i.f[n](e,t),t)),[])),i.u=e=>(({78:"page_action-aggregate",147:"metrics-aggregate",242:"session-manager",317:"jserrors-aggregate",348:"page_view_timing-aggregate",412:"lazy-feature-loader",439:"async-api",538:"recorder",590:"session_replay-aggregate",675:"compressor",733:"session_trace-aggregate",786:"page_view_event-aggregate",873:"spa-aggregate",898:"ajax-aggregate"}[e]||e)+"."+{78:"ac76d497",147:"3dc53903",148:"1a20d5fe",242:"2a64278a",317:"49e41428",348:"bd6de33a",412:"2f55ce66",439:"30bd804e",538:"1b18459f",590:"cf0efb30",675:"ae9f91a8",733:"83105561",786:"06482edd",860:"03a8b7a5",873:"e6b09d52",898:"998ef92b"}[e]+"-1.236.0.min.js"),i.o=(e,t)=>Object.prototype.hasOwnProperty.call(e,t),e={},t="NRBA:",i.l=(n,r,o,a)=>{if(e[n])e[n].push(r);else{var s,c;if(void 0!==o)for(var d=document.getElementsByTagName("script"),u=0;u<d.length;u++){var l=d[u];if(l.getAttribute("src")==n||l.getAttribute("data-webpack")==t+o){s=l;break}}s||(c=!0,(s=document.createElement("script")).charset="utf-8",s.timeout=120,i.nc&&s.setAttribute("nonce",i.nc),s.setAttribute("data-webpack",t+o),s.src=n),e[n]=[r];var f=(t,r)=>{s.onerror=s.onload=null,clearTimeout(g);var i=e[n];if(delete e[n],s.parentNode&&s.parentNode.removeChild(s),i&&i.forEach((e=>e(r))),t)return t(r)},g=setTimeout(f.bind(null,void 0,{type:"timeout",target:s}),12e4);s.onerror=f.bind(null,s.onerror),s.onload=f.bind(null,s.onload),c&&document.head.appendChild(s)}},i.r=e=>{"undefined"!=typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(e,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(e,"__esModule",{value:!0})},i.j=6,i.p="https://js-agent.newrelic.com/",(()=>{var e={6:0,352:0};i.f.j=(t,n)=>{var r=i.o(e,t)?e[t]:void 0;if(0!==r)if(r)n.push(r[2]);else{var o=new Promise(((n,i)=>r=e[t]=[n,i]));n.push(r[2]=o);var a=i.p+i.u(t),s=new Error;i.l(a,(n=>{if(i.o(e,t)&&(0!==(r=e[t])&&(e[t]=void 0),r)){var o=n&&("load"===n.type?"missing":n.type),a=n&&n.target&&n.target.src;s.message="Loading chunk "+t+" failed.\n("+o+": "+a+")",s.name="ChunkLoadError",s.type=o,s.request=a,r[1](s)}}),"chunk-"+t,t)}};var t=(t,n)=>{var r,o,[a,s,c]=n,d=0;if(a.some((t=>0!==e[t]))){for(r in s)i.o(s,r)&&(i.m[r]=s[r]);if(c)c(i)}for(t&&t(n);d<a.length;d++)o=a[d],i.o(e,o)&&e[o]&&e[o][0](),e[o]=0},n=window.webpackChunkNRBA=window.webpackChunkNRBA||[];n.forEach(t.bind(null,0)),n.push=t.bind(null,n.push.bind(n))})();var o={};(()=>{i.r(o);var e=i(3325),t=i(5763);const n=Object.values(e.D);function r(e){const r={};return n.forEach((n=>{r[n]=function(e,n){return!1!==(0,t.Mt)(n,"".concat(e,".enabled"))}(n,e)})),r}var a=i(9144);var s=i(5546),c=i(385),d=i(8e3),u=i(5938),l=i(3960),f=i(50);class g extends u.W{constructor(e,t,n){let r=!(arguments.length>3&&void 0!==arguments[3])||arguments[3];super(e,t,n),this.auto=r,this.abortHandler,this.featAggregate,this.onAggregateImported,r&&(0,d.R)(e,n)}importAggregator(){let e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{};if(this.featAggregate||!this.auto)return;const n=c.il&&!0===(0,t.Mt)(this.agentIdentifier,"privacy.cookies_enabled");let r;this.onAggregateImported=new Promise((e=>{r=e}));const o=async()=>{let t;try{if(n){const{setupAgentSession:e}=await Promise.all([i.e(860),i.e(242)]).then(i.bind(i,3228));t=e(this.agentIdentifier)}}catch(e){(0,f.Z)("A problem occurred when starting up session manager. This page will not start or extend any session.",e)}try{if(!this.shouldImportAgg(this.featureName,t))return void(0,d.L)(this.agentIdentifier,this.featureName);const{lazyFeatureLoader:n}=await i.e(412).then(i.bind(i,8582)),{Aggregate:o}=await n(this.featureName,"aggregate");this.featAggregate=new o(this.agentIdentifier,this.aggregator,e),r(!0)}catch(e){(0,f.Z)("Downloading and initializing ".concat(this.featureName," failed..."),e),this.abortHandler?.(),r(!1)}};c.il?(0,l.b)((()=>o()),!0):o()}shouldImportAgg(n,r){return n!==e.D.sessionReplay||!1!==(0,t.Mt)(this.agentIdentifier,"session_trace.enabled")&&(!!r?.isNew||!!r?.state.sessionReplay)}}var p=i(7633),h=i(7894);class v extends g{static featureName=p.t9;constructor(n,r){let i=!(arguments.length>2&&void 0!==arguments[2])||arguments[2];if(super(n,r,p.t9,i),("undefined"==typeof PerformanceNavigationTiming||c.Tt)&&"undefined"!=typeof PerformanceTiming){const r=(0,t.OP)(n);r[p.Dz]=Math.max(Date.now()-r.offset,0),(0,l.K)((()=>r[p.qw]=Math.max((0,h.z)()-r[p.Dz],0))),(0,l.b)((()=>{const t=(0,h.z)();r[p.OJ]=Math.max(t-r[p.Dz],0),(0,s.p)("timing",["load",t],void 0,e.D.pageViewTiming,this.ee)}))}this.importAggregator()}}var m=i(1117),b=i(1284);class y extends m.w{constructor(e){super(e),this.aggregatedData={}}store(e,t,n,r,i){var o=this.getBucket(e,t,n,i);return o.metrics=function(e,t){t||(t={count:0});return t.count+=1,(0,b.D)(e,(function(e,n){t[e]=w(n,t[e])})),t}(r,o.metrics),o}merge(e,t,n,r,i){var o=this.getBucket(e,t,r,i);if(o.metrics){var a=o.metrics;a.count+=n.count,(0,b.D)(n,(function(e,t){if("count"!==e){var r=a[e],i=n[e];i&&!i.c?a[e]=w(i.t,r):a[e]=function(e,t){if(!t)return e;t.c||(t=A(t.t));return t.min=Math.min(e.min,t.min),t.max=Math.max(e.max,t.max),t.t+=e.t,t.sos+=e.sos,t.c+=e.c,t}(i,a[e])}}))}else o.metrics=n}storeMetric(e,t,n,r){var i=this.getBucket(e,t,n);return i.stats=w(r,i.stats),i}getBucket(e,t,n,r){this.aggregatedData[e]||(this.aggregatedData[e]={});var i=this.aggregatedData[e][t];return i||(i=this.aggregatedData[e][t]={params:n||{}},r&&(i.custom=r)),i}get(e,t){return t?this.aggregatedData[e]&&this.aggregatedData[e][t]:this.aggregatedData[e]}take(e){for(var t={},n="",r=!1,i=0;i<e.length;i++)t[n=e[i]]=x(this.aggregatedData[n]),t[n].length&&(r=!0),delete this.aggregatedData[n];return r?t:null}}function w(e,t){return null==e?function(e){e?e.c++:e={c:1};return e}(t):t?(t.c||(t=A(t.t)),t.c+=1,t.t+=e,t.sos+=e*e,e>t.max&&(t.max=e),e<t.min&&(t.min=e),t):{t:e}}function A(e){return{t:e,min:e,max:e,sos:e*e,c:1}}function x(e){return"object"!=typeof e?[]:(0,b.D)(e,D)}function D(e,t){return t}var _=i(8632),j=i(4402),E=i(4351);var k=i(7956),T=i(3239),I=i(9251);class N extends g{static featureName=I.t;constructor(e,n){let r=!(arguments.length>2&&void 0!==arguments[2])||arguments[2];super(e,n,I.t,r),c.il&&((0,t.OP)(e).initHidden=Boolean("hidden"===document.visibilityState),(0,k.N)((()=>(0,s.p)("docHidden",[(0,h.z)()],void 0,I.t,this.ee)),!0),(0,T.bP)("pagehide",(()=>(0,s.p)("winPagehide",[(0,h.z)()],void 0,I.t,this.ee))),this.importAggregator())}}var P=i(3081);class S extends g{static featureName=P.t9;constructor(e,t){let n=!(arguments.length>2&&void 0!==arguments[2])||arguments[2];super(e,t,P.t9,n),this.importAggregator()}}new class{constructor(e){let t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:(0,j.ky)(16);c._A?(this.agentIdentifier=t,this.sharedAggregator=new y({agentIdentifier:this.agentIdentifier}),this.features={},this.desiredFeatures=new Set(e.features||[]),this.desiredFeatures.add(v),Object.assign(this,(0,a.j)(this.agentIdentifier,e,e.loaderType||"agent")),this.start()):(0,f.Z)("Failed to initial the agent. Could not determine the runtime environment.")}get config(){return{info:(0,t.C5)(this.agentIdentifier),init:(0,t.P_)(this.agentIdentifier),loader_config:(0,t.DL)(this.agentIdentifier),runtime:(0,t.OP)(this.agentIdentifier)}}start(){const t="features";try{const n=r(this.agentIdentifier),i=[...this.desiredFeatures];i.sort(((t,n)=>e.p[t.featureName]-e.p[n.featureName])),i.forEach((t=>{if(n[t.featureName]||t.featureName===e.D.pageViewEvent){const r=function(t){switch(t){case e.D.ajax:return[e.D.jserrors];case e.D.sessionTrace:return[e.D.ajax,e.D.pageViewEvent];case e.D.sessionReplay:return[e.D.sessionTrace];case e.D.pageViewTiming:return[e.D.pageViewEvent];default:return[]}}(t.featureName);r.every((e=>n[e]))||(0,f.Z)("".concat(t.featureName," is enabled but one or more dependent features has been disabled (").concat((0,E.P)(r),"). This may cause unintended consequences or missing data...")),this.features[t.featureName]=new t(this.agentIdentifier,this.sharedAggregator)}})),(0,_.Qy)(this.agentIdentifier,this.features,t)}catch(e){(0,f.Z)("Failed to initialize all enabled instrument classes (agent aborted) -",e);for(const e in this.features)this.features[e].abortHandler?.();const n=(0,_.fP)();return delete n.initializedAgents[this.agentIdentifier]?.api,delete n.initializedAgents[this.agentIdentifier]?.[t],delete this.sharedAggregator,n.ee?.abort(),delete n.ee?.get(this.agentIdentifier),!1}}}({features:[v,N,S],loaderType:"lite"})})(),window.NRBA=o})();</script><meta name="viewport" content="width=device-width, initial-scale=1.0"> <!--[if IE]><![endif]--><link rel="dns-prefetch" href="//assets.adobedtm.com" /><link rel="preconnect" href="//assets.adobedtm.com" /><meta http-equiv="Content-Type" content="text/html; charset=utf-8" /><link rel="alternate" type="application/rss+xml" title="RSS - Chapter 24. The Importance of Being Linear" href="https://developer.nvidia.com/taxonomy/term/1438/feed" /><link rel="shortcut icon" href="https://developer.nvidia.com/sites/all/themes/devzone_base/favicon.ico" type="image/vnd.microsoft.icon" /><meta name="description" content="Chapter 24. The Importance of Being Linear Larry Gritz NVIDIA Corporation Eugene d&#039;Eon NVIDIA Corporation 24.1 Introduction The performance and programmability of modern GPUs allow highly realistic lighting and shading to be achieved in real time. However, a subtle nonlinear property of almost every device that captures or displays digital images necessitates careful" /><meta name="generator" content="Drupal 7 (http://drupal.org)" /><link rel="canonical" href="https://developer.nvidia.com/gpugems/gpugems3/part-iv-image-effects/chapter-24-importance-being-linear" /><link rel="shortlink" href="https://developer.nvidia.com/taxonomy/term/1438" /><meta property="og:site_name" content="NVIDIA Developer" /><meta property="og:type" content="article" /><meta property="og:url" content="https://developer.nvidia.com/gpugems/gpugems3/part-iv-image-effects/chapter-24-importance-being-linear" /><meta property="og:title" content="Chapter 24. The Importance of Being Linear" /><meta property="og:description" content="Chapter 24. The Importance of Being Linear Larry Gritz NVIDIA Corporation Eugene d&#039;Eon NVIDIA Corporation 24.1 Introduction The performance and programmability of modern GPUs allow highly realistic lighting and shading to be achieved in real time. However, a subtle nonlinear property of almost every device that captures or displays digital images necessitates careful processing of textures and frame buffers to ensure that all this lighting and shading is computed and displayed correctly. Proper gamma correction is probably the easiest, most inexpensive, and most widely applicable technique for improving image quality in real-time applications. 24.2 Light, Displays, and Color Spaces 24.2.1 Problems with Digital Image Capture, Creation, and Display If you are interested in high-quality rendering, you might wonder if the images displayed on your CRT, LCD, film, or paper will result in light patterns that are similar enough to the &quot;real world&quot; situation so that your eye will perceive them as realistic. You may be surprised to learn that there are several steps in the digital image creation pipeline where things can go awry. In particular: Does the capture of the light (by a camera, scanner, and so on) result in a digital image whose numerical values accurately represent the relative light levels that were present? If twice as many photons hit the sensor, will its numerical value be twice as high? Does a producer of synthetic images, such as a renderer, create digital images whose pixel values are proportional to what light would really do in the situation they are simulating? Does the display accurately turn the digital image back into light? If a pixel&#039;s numerical value is doubled, will the CRT, LCD, or other similar device display an image that appears twice as bright? The answer to these questions is, surprisingly, probably not. In particular, both the capture (scanning, painting, and digital photography) and the display (CRT, LCD, or other) are likely not linear processes, and this can lead to incorrect and unrealistic images if care is not taken at these two steps of the pipeline. The nonlinearity is subtle enough that it is often unintentionally or intentionally ignored, particularly in real-time graphics. However, its effects on rendering, particularly in scenes with plenty of dynamic range like Figure 24-1, are quite noticeable, and the simple steps needed to correct it are well worth the effort. Figure 24-1 The Benefit of Proper Gamma Correction 24.2.2 Digression: What Is Linear? In the mathematical sense, a linear transformation is one in which the relationship between inputs and outputs is such that: The output of the sum of inputs is equal to the sum of the outputs of the individual inputs. That is, f (x + y) = f (x) + f (y). The output scales by the same factor as a scale of the input (for a scalar k): f(k x x) = k x f (x). Light transport is linear. The illumination contributions from two light sources in a scene will sum. They will not multiply, subtract, or interfere with each other in unobvious ways. [1] 24.2.3 Monitors Are Nonlinear, Renderers Are Linear CRTs do not behave linearly in their conversion of voltages into light intensities. And LCDs, although they do not inherently have this property, are usually constructed to mimic the response of CRTs. A monitor&#039;s response is closer to an exponential curve, as shown in Figure 24-2, and the exponent is called gamma. A typical gamma of 2.2 means that a pixel at 50 percent intensity emits less than a quarter of the light as a pixel at 100 percent intensity—not half, as you would expect! Gamma is different for every individual display device, [2] but typically it is in the range of 2.0 to 2.4. Adjusting for the effects of this nonlinear characteristic is called gamma correction. Figure 24-2 Typical Response Curve of a Monitor Note that regardless of the exponent applied, the values of black (zero) and white (one) will always be the same. It&#039;s the intermediate, midtone values that will be corrected or distorted. Renderers, shaders, and compositors like to operate with linear data. They sum the contributions of multiple light sources and multiply light values by reflectances (such as a constant value or a lookup from a texture that represents diffuse reflectivity). But there are hidden assumptions—such as, that a texture map that indicates how reflectivity varies over the surface also has a linear scale. Or that, upon display, the light intensities seen by the viewer will be indeed proportional to the values the renderer stored in the frame buffer. The point is, if you have nonlinear inputs, then a renderer, shader, or compositor will &quot;do the math wrong.&quot; This is an impediment to achieving realistic results. Consider a digital painting. If you weren&#039;t viewing with &quot;proper gamma correction&quot; while you were creating the painting, the painting will contain &quot;hidden&quot; nonlinearities. The painting may look okay on your monitor, but it may appear different on other monitors and may not appear as you expected if it&#039;s used in a 3D rendering. The renderer will not actually use the values correctly if it assumes the image is linear. Also, if you take the implicitly linear output of a renderer and display it without gamma correction, the result will look too dark—but not uniformly too dark, so it&#039;s not enough to just make it brighter. If you err on both ends of the imaging pipeline—you paint or capture images in gamma (nonlinear) space, render linear, and then fail to correct when displaying the rendered images—the results might look okay at first glance. Why? Because the nonlinear painting/capture and the nonlinear display may cancel each other out to some degree. But there will be subtle artifacts riddled through the rendering and display process, including colors that change if a nonlinear input is brightened or darkened by lighting, and alpha-channel values that are wrong (compositing artifacts), and so your mipmaps were made wrong (texture artifacts). Plus, any attempts at realistic lighting—such as high dynamic range (HDR) and imaged-based lighting—are not really doing what you expect. Also, your results will tend to look different for everybody who displays them because the paintings and the output have both, in some sense, a built-in reference to a particular monitor—and not necessarily the same monitors will be used for creating the paintings and 3D lighting! 24.3 The Symptoms If you ignore the problem—paint or light in monitor space and display in monitor space—you may encounter the following symptoms. 24.3.1 Nonlinear Input Textures The average user doesn&#039;t have a calibrated monitor and has never heard of gamma correction; therefore, many visual materials are precorrected for them. For example, by convention, all JPEG files are precorrected for a gamma of 2.2. That&#039;s not exact for any monitor, but it&#039;s in the ballpark, so the image will probably look acceptable on most monitors. This means that JPEG images (including scans and photos taken with a digital camera) are not linear, so they should not be used as texture maps by shaders that assume linear input. This precorrected format is convenient for directly displaying images on the average LCD or CRT display. And for storage of 8-bit images, it affords more &quot;color resolution&quot; in the darks, where the eye is more sensitive to small gradations of intensity. However, this format requires that these images be processed before they are used in any kind of rendering or compositing operation. 24.3.2 Mipmaps When creating a mipmap, the obvious way to downsize at each level is for each lower-mip texel to be one-fourth the sum of the four texels &quot;above&quot; it at the higher resolution. Suppose at the high-resolution level you have an edge: two pixels at 1.0 and two at 0.0. The low-resolution pixel ought to be 0.5, right? Half as bright, because it&#039;s half fully bright and half fully dark? And that&#039;s surely how you&#039;re computing the mipmap. Aha, but that&#039;s linear math. If you are in a nonlinear color space with a gamma, say of 2.0, then that coarse-level texel with a value of 0.5 will be displayed at only 25 percent of the brightness. So you will see not-so-subtle errors in low-resolution mipmap levels, where the intensities simply aren&#039;t the correctly filtered results of the high-resolution levels. The brightness of rendered objects will change along contour edges and with distance from the 3D camera. Furthermore, there is a subtle problem with texture-filtering nonlinear textures on the GPU (or even CPU renderers), even if you&#039;ve tried to correct for nonlinearities when creating the mipmaps. The GPU uses the texture lookup filter extents to choose a texture level of detail (LOD), or &quot;mipmap level,&quot; and will blend between two adjacent mipmap levels if necessary. We hope by now you&#039;ve gotten the gist of our argument and understand that if the render-time blend itself is also done assuming linear math—and if the two texture inputs are nonlinear—the results won&#039;t quite give the right color intensities. This situation will lead to textures subtly pulsating in intensity during transitions between mipmap levels. 24.3.3 Illumination Consider the illumination of a simple Lambertian sphere, as shown in Figure 24-3. [3] If the reflected light is proportional to N · L, then a spot A on the sphere where N and L form a 60-degree angle should reflect half the light toward the camera. Thus, it appears half as bright as a spot B on the sphere, where N points straight to the light source. If these image values were converted to light linearly, when you view the image it should look like a real object with those reflective properties. But if you display the image on a monitor with a gamma of 2.0, A will actually only be 0.5 gamma , or one-fourth as bright as B. Figure 24-3 A Linear Image Gamma-Corrected () and Uncorrected () In other words, computer-generated (CG) materials and lights will simply not match the appearance of real materials and lights if you render assuming linear properties of light but display on a nonlinear monitor. Overall, the scene will appear dimmer than an accurate simulation. However, merely brightening it by a constant factor is not good enough: The brights will be more correct than the midrange intensities. Shadow terminators and intensity transitions will be sharper—for example, the transition from light to dark will be faster—than in the real world. Corners will look too dark. And the more &quot;advanced&quot; lighting techniques that you use (such as HDR, global illumination of any kind, and subsurface scattering), the more critical it will become to stick to a linear color space to match the linear calculations of your sophisticated lighting. 24.3.4 Two Wrongs Don&#039;t Make a Right The most common gamma mistake made in rendering is using nonlinear color textures for shading and then not applying gamma correction to the final image. This double error is much less noticeable than making just one of these mistakes, because the corrections required for each are roughly opposites. However, this situation creates many problems that can be easily avoided. Figure 24-4 shows a comparison between two real-time renderings using a realistic skin shader. (For more information on skin shaders, see Chapter 14 of this book, &quot;Advanced Techniques for Realistic Real-Time Skin Rendering.&quot;) The left image converted the diffuse color texture into a linear space because the texture came from several JPEG photographs. Lighting (including subsurface scattering) and shading were performed correctly in a linear space and then the final image was gamma-corrected for display on average nonlinear display devices. Figure 24-4 Rendering with Proper Gamma Correction () and Rendering Ignoring Gamma () The image on the right made neither of these corrections and exhibits several problems. The skin tone from the color photographs is changed because the lighting was performed in a nonlinear space, and as the light brightened and darkened the color values, the colors were inadvertently changed. (The red levels of the skin tones are higher than the green and blue levels and thus receive a different boost when brightened or darkened by light and shadow.) The white specular light, when added to the diffuse lighting, becomes yellow. The shadowed regions become too dark, and the subsurface scattering (particularly the subtle red diffusion into shadowed regions) is almost totally missing because it is squashed by the gamma curve when it&#039;s displayed. Adjusting lighting becomes problematic in a nonlinear space. If we take the same scenes in Figure 24-4 and render them now with the light intensities increased, the problems in the nonlinear version become worse, as you can see in Figure 24-5. As a rule, if color tones change as the same object is brightened or darkened, then most likely nonlinear lighting is taking place and gamma correction is needed. Figure 24-5 Rendering with Proper Gamma Correction () and Rendering Incorrect Gamma () A common problem encountered when subsurface scattering is used for skin rendering (when gamma correction is ignored) is the appearance of a blue-green glow around the shadow edges and an overly waxy-looking skin, as shown in Figure 24-6. These problems arise when the scattering parameters are adjusted to give shadow edges the desired amount of red bleeding (as seen in Figure 24-5). This effect is hard to achieve in a nonlinear color space and requires a heavily weighted broad blur of irradiance in the red channel. The result causes far too much diffusion in the bright regions of the face (giving a waxy look) and causes the red to darken too much as it approaches a shadow edge (leaving a blue-green glow). Figure 24-6 Tweaking Subsurface Scattering When Rendering Without Gamma Correction Is Problematic 24.4 The Cure Gamma correction is the practice of applying the inverse of the monitor transformation to the image pixels before they&#039;re displayed. That is, if we raise pixel values to the power 1/gamma before display, then the display implicitly raising to the power gamma will exactly cancel it out, resulting, overall, in a linear response (see Figure 24-2). The usual implementation is to have your windowing systems apply color-correction lookup tables (LUTs) that incorporate gamma correction. This adjustment allows all rendering, compositing, or other image manipulation to use linear math and for any linear-scanned images, paintings, or rendered images to be displayed correctly. Animation and visual effects studios, as well as commercial publishers, are very careful with this process. Often it&#039;s the exclusive job of a staff person to understand color management for their monitors, compositing pipeline, film and video scanning, and final outputs. In fact, for high-end applications, simple gamma correction is not enough. Often compositors use a much more sophisticated 3D color LUT derived from careful measurements of the individual displays or film stocks&#039; color-response curves. [4] In contrast to visual effects and animation for film, game developers often get this process wrong, which leads to the artifacts discussed in this chapter. This is one reason why most (but not all) CG for film looks much better than games—a reason that has nothing to do with the polygon counts, shading, or artistic skills of game creators. (It&#039;s also sometimes a reason why otherwise well-made film CG looks poor—because the color palettes and gammas have been mismatched by a careless compositor.) Joe GamePlayer doesn&#039;t have a calibrated monitor. We don&#039;t know the right LUTs to apply for his display, and in any case, he doesn&#039;t want to apply gamma-correcting LUTs to his entire display. Why? Because then the ordinary JPEG files viewed with Web browsers and the like will look washed out and he won&#039;t understand why. (Film studios don&#039;t care if random images on the Internet look wrong on an artist&#039;s workstation, as long as their actual work product—the film itself—looks perfect in the theatre, where they have complete control.) But simple gamma correction for the &quot;average&quot; monitor can get us most of the way there. The remainder of this chapter will present the easiest solutions to improve and avoid these artifacts in games you are developing. 24.4.1 Input Images (Scans, Paintings, and Digital Photos) Most images you capture with scanning or digital photography are likely already gamma-corrected (especially if they are JPEGs by the time they get to you), and therefore are in a nonlinear color space. (Digital camera JPEGs are usually sharpened by the camera as well for capturing textures; try to use a RAW file format to avoid being surprised.) If you painted textures without using a gamma LUT, those paintings will also be in monitor color space. If an image looks right on your monitor in a Web browser, chances are it has been gamma-corrected and is nonlinear. Any input textures that are already gamma-corrected need to be brought back to a linear color space before they can be used for shading or compositing. You want to make this adjustment for texture values that are used for (linear) lighting calculations. Color values (such as light intensities and diffuse reflectance colors) should always be uncorrected to a linear space before they&#039;re used in shading. Alpha channels, normal maps, displacement values (and so on) are almost certainly already linear and should not be corrected further, nor should any textures that you were careful to paint or capture in a linear fashion. All modern GPUs support sRGB texture formats. These formats allow binding gamma-corrected pixel data directly as a texture with proper gamma applied by the hardware before the results of texture fetches are used for shading. On NVIDIA GeForce 8-class (and future) hardware, all samples used in a texture filtering operation are linearized before filtering to ensure the proper filtering is performed (older GPUs apply the gamma post-filtering). The correction applied is an IEC standard (IEC 61966-2-1) that corresponds to a gamma of roughly 2.2 and is a safe choice for nonlinear color data where the exact gamma curve is not known. Alpha values, if present, are not corrected. Appropriate sRGB formats are defined for all 8-bit texture formats (RGB, RGBA, luminance, luminance alpha, and DXT compressed), both in OpenGL and in DirectX. Passing GL_SRGB_EXT instead of GL_RGB to glTexImage2D, for example, ensures that any shader accesses to the specified texture return linear pixel values. The automatic sRGB corrections are free and are preferred to performing the corrections manually in a shader after each texture access, as shown in Listing 24-1, because each pow instruction is scalar and expanded to two instructions. Also, manual correction happens after filtering, which is incorrectly performed in a nonlinear space. The sRGB formats may also be preferred to preconverting textures to linear versions before they are loaded. Storing linear pixels back into an 8-bit image is effectively a loss of precision in low light levels and can cause banding when the pixels are converted back to monitor space or used in further shading. Example 24-1. Manually Converting Color Values to a Linear Space Texture lookups can apply inverse gamma correction so that the rest of your shader is working with linear values. However, using an sRGB texture is faster, allows proper linear texture filtering (GeForce 8 and later), and requires no extra shader code. float3 diffuseCol = pow( f3tex2D( diffTex, texCoord ), 2.2 ); // Or (cheaper, but assuming gamma of 2.0 rather than 2.2) float3 diffuseCol = f3tex2D( diffTex, texCoord ); diffuseCol = diffuseCol * diffuseCol; Managing shaders that need to mix linear and nonlinear inputs can be an ugly logistical chore for the engine programmers and artists who provide textures. The simplest solution, in many cases, is to simply require all textures to be precorrected to linear space before they&#039;re delivered for use in rendering. 24.4.2 Output Images (Final Renders) The last step before display is to gamma-correct the final pixel values so that when they&#039;re displayed on a monitor with nonlinear response, the image looks &quot;correct.&quot; Specifying an sRGB frame buffer leaves the correction to the GPU, and no changes to shaders are required. Any value returned in the shader is gamma-corrected before storage in the frame buffer (or render-to-texture buffer). Furthermore, on GeForce 8-class and later hardware, if blending is enabled, the previously stored value is converted back to linear before blending and the result of the blend is gamma-corrected. Alpha values are not gamma-corrected when sRGB buffers are enabled. If sRGB buffers are not available, you can use the more costly solution of custom shader code, as shown in Listing 24-2; however, any blending, if enabled, will be computed incorrectly. Example 24-2. Last-Stage-Output Gamma Correction If sRGB frame buffers are not available (or if a user-defined gamma value is exposed), the following code will perform gamma correction. float3 finalCol = do_all_lighting_and_shading(); float pixelAlpha = compute_pixel_alpha(); return float4(pow(finalCol, 1.0 / 2.2), pixelAlpha); // Or (cheaper, but assuming gamma of 2.0 rather than 2.2) return float4( sqrt( finalCol ), pixelAlpha ); 24.4.3 Intermediate Color Buffers A few subtle points should be kept in mind. If you are doing any kind of post-processing pass on your images, you should be doing the gamma correction as the last step of the last post-processing pass. Don&#039;t render, correct, and then do further math on the result as if it were a linear image. Also, if you are rendering to create a texture, you need to either (a) gamma-correct, and then treat the texture as a nonlinear input when performing any further processing of it, or (b) not gamma-correct, and treat the texture as linear input for any further processing. Intermediate color buffers may lose precision in the darks if stored as 8-bit linear images, compared to the precision they would have as gamma-corrected images. Thus, it may be beneficial to use 16-bit floating-point or sRGB frame-buffer and sRGB texture formats for rendering and accessing intermediate color buffers. 24.5 Conclusion OpenGL, DirectX, and any shaders you write are probably performing math as if all texture inputs, light/material interactions, and outputs are linear (that is, light intensities sum; diffuse reflectivities multiply). But it is very likely that your texture inputs may be nonlinear, and it&#039;s almost certain that your user&#039;s uncalibrated and uncorrected monitor applies a nonlinear color-space transformation. This scenario leads to all sorts of artifacts and inaccuracies, some subtle (such as mipmap filtering errors) and some grossly wrong (such as very incorrect light falloff). We strongly suggest that developers take the following simple steps: Assume that most game players are using uncalibrated, uncorrected monitors that can roughly be characterized by an exponential response with gamma = 2.2. (For an even higher-quality end-user experience: Have your game setup display a gamma calibration chart and let the user choose a good gamma value.) When performing lookups from nonlinear textures (those that look &quot;right&quot; on uncorrected monitors) that represent light or color values, raise the results to the power gamma to yield a linear value that can be used by your shader. Do not make this adjustment for values already in a linear color space, such as certain high-dynamic-range light maps or images containing normals, bump heights, or other noncolor data. Use sRGB texture formats, if possible, for increased performance and correct texture filtering on GeForce 8 GPUs (and later). Apply a gamma correction (that is, raise to power 1/gamma) to the final pixel values as the very last step before displaying them. Use sRGB frame-buffer extensions for efficient automatic gamma correction with proper blending. Carefully following these steps is crucial to improving the look of your game and especially to increasing the accuracy of any lighting or material calculations you are performing in your shaders. 24.6 Further Reading In this chapter, we&#039;ve tried to keep the descriptions and advice simple, with the intent of making a fairly nontechnical argument for why you should use linear color spaces. In the process, we&#039;ve oversimplified. For those of you who crave the gory details, an excellent treatment of the gamma problem can be found on Charles Poynton&#039;s Web page: http://www.poynton.com/GammaFAQ.html. The Wikipedia entry on gamma correction is surprisingly good: http://en.wikipedia.org/wiki/Gamma_correction. For details regarding sRGB hardware formats in OpenGL and DirectX, see these resources: http://www.nvidia.com/dev_content/nvopenglspecs/GL_EXT_texture_sRGB.txt http://www.opengl.org/registry/specs/EXT/framebuffer_sRGB.txt http://msdn2.microsoft.com/en-us/library/bb173460.aspx Thanks to Gary King and Mark Kilgard for their expertise on sRGB and their helpful comments regarding this chapter. Special thanks to actor Doug Jones for kindly allowing us to use his likeness. And finally, Figure 24-2 was highly inspired by a very similar version we found on Wikipedia." /><title>Chapter 24. The Importance of Being Linear | NVIDIA Developer</title><link type="text/css" rel="stylesheet" href="/sites/default/files/advagg_css/css__9cWqptSUNYq5YedfIwh33VxtugFVWjDdEsblT8GhLKI__BxLS2KgpjdifBIVLmihGT5aNKXGtbAg2BJGQczE8XZY__RdADzPRPOCdjuQ5t3WiEWu4q_r6oFSMi1EfqzG9BsYY.css" media="all" /><link type="text/css" rel="stylesheet" href="/sites/default/files/advagg_css/css__gzhCDL7s2DdMqxGYgWxUMcRJhFfthtVcV5dQFZdV090__yAkxBLUKwF6cHyMzviVeuMUXsvTJC8gS_7UFjY1cM_0__RdADzPRPOCdjuQ5t3WiEWu4q_r6oFSMi1EfqzG9BsYY.css" media="all" /><link type="text/css" rel="stylesheet" href="/sites/default/files/advagg_css/css__2R4Y9oq919N2i8xujQIMTaS0SL_HrIxMfYkgOmDPbjg__fhvJVXXQ_0VSZPsugyxWP4moufN-95EZrJc4g5Xp1vE__RdADzPRPOCdjuQ5t3WiEWu4q_r6oFSMi1EfqzG9BsYY.css" media="screen" /><link type="text/css" rel="stylesheet" href="/sites/default/files/advagg_css/css__a3ZVN5Xj0R9qi_ZvAUJOf6WSwyQEZA3Pz3p-Z3XUydA__iit9VWZZIR82Y45kzaDBtwDd83cxMde52Ei4AckqIqE__RdADzPRPOCdjuQ5t3WiEWu4q_r6oFSMi1EfqzG9BsYY.css" media="all" /><link type="text/css" rel="stylesheet" href="/sites/default/files/advagg_css/css__QbJBCTfEkw5tuChaE_CRiF6ubU_MK-2NJPMQC4iq1AI__0xYzyHNWGr1nhNXBNea0TGkY6S2pTbE5O_MgRzHwTcM__RdADzPRPOCdjuQ5t3WiEWu4q_r6oFSMi1EfqzG9BsYY.css" media="all" /><link type="text/css" rel="stylesheet" href="https://developer.nvidia.com/register/dz-auth.css?v=3.2.5c" media="all" /><link type="text/css" rel="stylesheet" href="/sites/default/files/advagg_css/css__IqF7j_6rmPbc_Qj1mirsNBUMNx07CDpDVTwr8UHaDAo__rhBTn3zKWeLiY1O6TLx8SVT-E3hRD4nfJuNeoQva-X0__RdADzPRPOCdjuQ5t3WiEWu4q_r6oFSMi1EfqzG9BsYY.css" media="all" /><link type="text/css" rel="stylesheet" href="/sites/default/files/advagg_css/css__Ckr81K7j4rBvBws4lzDQewUtq4fggnBYGaEJ-eT0PRQ__A61hNA7jsaNTD1H4NpWKPHOyv6COeJ7Zc2oCFtMR5B4__RdADzPRPOCdjuQ5t3WiEWu4q_r6oFSMi1EfqzG9BsYY.css" media="all" /> <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]--> <script type="text/javascript" src="https://cdn.cookielaw.org/consent/90e816a3-9ed2-44eb-9c16-80c5660aa8c8/OtAutoBlock.js" ></script><script src="https://cdn.cookielaw.org/scripttemplates/otSDKStub.js" data-document-language="true" type="text/javascript" charset="UTF-8" data-domain-script="90e816a3-9ed2-44eb-9c16-80c5660aa8c8" ></script><script type="text/javascript">
function OptanonWrapper() { }
</script><script type="text/javascript" src="/sites/default/files/advagg_js/js__vmePF4Dp8bPkg81UKngZykilw5OZS1fP5YnScEtCamY__kbUIoZY5G0-ENLEHrfeVW2ZNHoxXsOBC04ZLARuReOY__RdADzPRPOCdjuQ5t3WiEWu4q_r6oFSMi1EfqzG9BsYY.js"></script><script type="text/javascript" src="/sites/default/files/advagg_js/js__l10lNRz-yUk-nXi4EOh5ZnASIEz4W-QRDYc-VZMD5P8__ken2CELa8wA-HfKc8lp0rOPlEJ-ccRi3B4L1VfwlCDs__RdADzPRPOCdjuQ5t3WiEWu4q_r6oFSMi1EfqzG9BsYY.js"></script><script type="text/javascript" data-ot-ignore="true" src="//assets.adobedtm.com/5d4962a43b79/c1061d2c5e7b/launch-191c2462b890.min.js"></script><script type="text/javascript" src="/sites/default/files/advagg_js/js__qGDNlcVWJA_anooobAs9NSHzCX2mC1WfU1R7fvC5cbk__eyX1g8QoN8bT7a1j-sqlmYmjSx-wB0CGQ5L9FyUpxMc__RdADzPRPOCdjuQ5t3WiEWu4q_r6oFSMi1EfqzG9BsYY.js"></script><script type="text/javascript" src="/sites/default/files/advagg_js/js__5mCF1AzRhjKySXlx-jTQmThW88GiAILClEA0dxswV_8__ZWuQFr0eYSgxqHjRKbjVu-9F7XlXHyFkJtAJRhIogAA__RdADzPRPOCdjuQ5t3WiEWu4q_r6oFSMi1EfqzG9BsYY.js"></script><script type="text/javascript" src="/sites/default/files/advagg_js/js__uDlbj2_6L83OHa8B4JTOctmpseoofmKPMV0hW-i6H2w__zqwOTDk9JQxAGenguBvWMMZ7eVijalwjNosOiFpjTMQ__RdADzPRPOCdjuQ5t3WiEWu4q_r6oFSMi1EfqzG9BsYY.js"></script><script type="text/javascript">
<!--//--><![CDATA[//><!--
localStorage.removeItem("STYXKEY_nv_jwt");
//--><!]]>
</script><script type="text/javascript" src="/sites/default/files/advagg_js/js__UEK_qHM4s4NXJe9qw6ROcizho2bgweItr5Ry1KZK3do__LHlNnsUSfTHKxCCpvHTqkGOE-Se725uZHx3j-VjoABg__RdADzPRPOCdjuQ5t3WiEWu4q_r6oFSMi1EfqzG9BsYY.js"></script><script type="text/javascript" src="/sites/default/files/advagg_js/js__CwAMXQhrwYglrvCYjKHSsLsaR0dUVMml9mR4sTsWBvE__k32l1vxJqSxIBmoomHdXBq2HF7nnDea1sMITQ848JYQ__RdADzPRPOCdjuQ5t3WiEWu4q_r6oFSMi1EfqzG9BsYY.js"></script><script type="text/javascript">
<!--//--><![CDATA[//><!--
jQuery.extend(Drupal.settings,{"basePath":"\/","pathPrefix":"","setHasJsCookie":0,"ajaxPageState":{"theme":"devzone_base","theme_token":"WDB7dO9nTEV6EDPQuXbtr20pWIXkPebDINWP9a_KSX4","css":{"modules\/system\/system.base.css":1,"misc\/ui\/jquery.ui.core.css":1,"misc\/ui\/jquery.ui.theme.css":1,"misc\/ui\/jquery.ui.button.css":1,"misc\/ui\/jquery.ui.resizable.css":1,"misc\/ui\/jquery.ui.dialog.css":1,"sites\/all\/modules\/contrib\/codefilter\/codefilter.css":1,"sites\/all\/modules\/contrib\/date\/date_api\/date.css":1,"sites\/all\/modules\/contrib\/date\/date_popup\/themes\/datepicker.1.7.css":1,"sites\/all\/modules\/contrib\/date\/date_repeat_field\/date_repeat_field.css":1,"modules\/field\/theme\/field.css":1,"modules\/node\/node.css":1,"sites\/all\/modules\/custom\/nvidia_quick_survey\/nvidia_quick_survey.css":1,"sites\/all\/modules\/contrib\/views\/css\/views.css":1,"sites\/all\/modules\/contrib\/colorbox\/styles\/default\/colorbox_style.css":1,"sites\/all\/modules\/contrib\/ctools\/css\/ctools.css":1,"sites\/all\/modules\/custom\/nvidia_tokens\/css\/nvidia-charts.css":1,"sites\/all\/modules\/custom\/nvidia_editor\/css\/nvidia_cookie_compliance.css":1,"sites\/all\/modules\/custom\/nvidia_editor\/css\/alter-one-trust-dom.css":1,"https:\/\/developer.nvidia.com\/register\/dz-auth.css?v=3.2.5c":1,"sites\/all\/modules\/contrib\/addtoany\/addtoany.css":1,"sites\/all\/modules\/custom\/nvidia_components\/css\/navigation-component.css":1,"sites\/all\/libraries\/devzone-shared-components\/dist\/nv-developer-menu.css":1,"sites\/all\/themes\/bootstrap\/css\/overrides.css":1,"sites\/all\/themes\/devzone_base\/css\/application.css":1,"sites\/all\/themes\/devzone_base\/css\/datatables.min.css":1},"js":{"sites\/all\/themes\/bootstrap\/js\/bootstrap.js":1,"sites\/all\/modules\/custom\/nvidia_tokens\/js\/horizontal-charts-init.js":1,"sites\/all\/modules\/custom\/nvidia_tokens\/js\/underscore.min.js":1,"sites\/all\/modules\/contrib\/jquery_update\/replace\/jquery\/1.10\/jquery.min.js":1,"misc\/jquery-extend-3.4.0.js":1,"misc\/jquery-html-prefilter-3.5.0-backport.js":1,"misc\/jquery.once.js":1,"misc\/drupal.js":1,"sites\/all\/modules\/contrib\/jquery_update\/replace\/ui\/ui\/minified\/jquery.ui.core.min.js":1,"sites\/all\/modules\/contrib\/jquery_update\/replace\/ui\/ui\/minified\/jquery.ui.widget.min.js":1,"sites\/all\/modules\/contrib\/jquery_update\/replace\/ui\/ui\/minified\/jquery.ui.button.min.js":1,"sites\/all\/modules\/contrib\/jquery_update\/replace\/ui\/ui\/minified\/jquery.ui.mouse.min.js":1,"sites\/all\/modules\/contrib\/jquery_update\/replace\/ui\/ui\/minified\/jquery.ui.draggable.min.js":1,"sites\/all\/modules\/contrib\/jquery_update\/replace\/ui\/ui\/minified\/jquery.ui.position.min.js":1,"misc\/ui\/jquery.ui.position-1.13.0-backport.js":1,"sites\/all\/modules\/contrib\/jquery_update\/replace\/ui\/ui\/minified\/jquery.ui.resizable.min.js":1,"sites\/all\/modules\/contrib\/jquery_update\/replace\/ui\/ui\/minified\/jquery.ui.dialog.min.js":1,"misc\/ui\/jquery.ui.dialog-1.13.0-backport.js":1,"sites\/all\/modules\/contrib\/eu_cookie_compliance\/js\/jquery.cookie-1.4.1.min.js":1,"\/\/assets.adobedtm.com\/5d4962a43b79\/c1061d2c5e7b\/launch-191c2462b890.min.js":1,"sites\/all\/libraries\/devzone-shared-components\/dist\/nv-developer-menu.js":1,"sites\/all\/modules\/contrib\/codefilter\/codefilter.js":1,"sites\/all\/modules\/custom\/nvidia_quick_survey\/nvidia_quick_survey.js":1,"sites\/all\/libraries\/colorbox\/jquery.colorbox-min.js":1,"sites\/all\/modules\/contrib\/colorbox\/js\/colorbox.js":1,"sites\/all\/modules\/contrib\/colorbox\/styles\/default\/colorbox_style.js":1,"sites\/all\/modules\/contrib\/colorbox\/js\/colorbox_load.js":1,"sites\/all\/modules\/contrib\/hint\/hint.js":1,"sites\/all\/modules\/custom\/nvidia_editor\/js\/js.cookie-2.2.1.min.js":1,"sites\/all\/modules\/custom\/nvidia_editor\/js\/munchkin.js":1,"sites\/all\/modules\/custom\/nvidia_editor\/js\/ls_track2.js":1,"sites\/all\/modules\/custom\/nvidia_editor\/js\/alter-one-trust-dom.js":1,"sites\/all\/modules\/custom\/nvidia_users\/js\/nvidia_users.cookie.js":1,"sites\/all\/themes\/devzone_base\/js\/jquery.migrate.min.js":1,"sites\/all\/themes\/devzone_base\/js\/jquery.isotope.js":1,"sites\/all\/themes\/devzone_base\/js\/jquery.sidr.js":1,"sites\/all\/themes\/devzone_base\/js\/datatables.min.js":1,"sites\/all\/themes\/devzone_base\/js\/application.js":1,"sites\/all\/themes\/devzone_base\/js\/attrchange.js":1,"sites\/all\/themes\/devzone_base\/js\/scripts.js":1,"sites\/all\/themes\/devzone_base\/bootstrap\/js\/affix.js":1,"sites\/all\/themes\/devzone_base\/bootstrap\/js\/alert.js":1,"sites\/all\/themes\/devzone_base\/bootstrap\/js\/button.js":1,"sites\/all\/themes\/devzone_base\/bootstrap\/js\/carousel.js":1,"sites\/all\/themes\/devzone_base\/bootstrap\/js\/collapse.js":1,"sites\/all\/themes\/devzone_base\/bootstrap\/js\/dropdown.js":1,"sites\/all\/themes\/devzone_base\/bootstrap\/js\/modal.js":1,"sites\/all\/themes\/devzone_base\/bootstrap\/js\/tooltip.js":1,"sites\/all\/themes\/devzone_base\/bootstrap\/js\/popover.js":1,"sites\/all\/themes\/devzone_base\/bootstrap\/js\/scrollspy.js":1,"sites\/all\/themes\/devzone_base\/bootstrap\/js\/tab.js":1,"sites\/all\/themes\/devzone_base\/bootstrap\/js\/transition.js":1,"sites\/all\/modules\/custom\/nvidia_tokens\/js\/d3.v4.min.js":1,"sites\/all\/modules\/custom\/nvidia_tokens\/js\/visualize-d.js":1}},"colorbox":{"opacity":"0.85","current":"{current} of {total}","previous":"\u00ab Prev","next":"Next \u00bb","close":"Close","maxWidth":"98%","maxHeight":"98%","fixed":true,"mobiledetect":true,"mobiledevicewidth":"480px","file_public_path":"\/sites\/default\/files","specificPagesDefaultValue":"admin*\nimagebrowser*\nimg_assist*\nimce*\nnode\/add\/*\nnode\/*\/edit\nprint\/*\nprintpdf\/*\nsystem\/ajax\nsystem\/ajax\/*"},"nvidia_site_search":{"is_cn_site":false},"nvDeveloperHeader":{"signedIn":false,"baseURL":"https:\/\/developer.nvidia.com","locale":"en-US","primaryMenu":{"links":[{"text":"Home","href":"https:\/\/developer.nvidia.com\/"},{"text":"Blog","href":"https:\/\/developer.nvidia.com\/blog\/"},{"text":"Forums","href":"https:\/\/forums.developer.nvidia.com\/"},{"text":"Docs","href":"https:\/\/docs.nvidia.com\/"},{"text":"Downloads","href":"https:\/\/developer.nvidia.com\/downloads"},{"text":"Training","href":"https:\/\/www.nvidia.com\/en-us\/deep-learning-ai\/education\/"}]},"secondaryMenu":{"links":[{"title":"Solutions","sections":[{"name":"AI and Data Science","links":[{"text":"Conversational AI","href":"https:\/\/developer.nvidia.com\/riva"},{"text":"Deep Learning","href":"https:\/\/developer.nvidia.com\/deep-learning"},{"text":"Inference","href":"https:\/\/developer.nvidia.com\/ai-inference-software"},{"text":"Machine Learning","href":"https:\/\/developer.nvidia.com\/machine-learning"},{"text":"Federated Learning","href":"https:\/\/developer.nvidia.com\/flare"},{"text":"AI-Enabled Video Analytics","href":"https:\/\/developer.nvidia.com\/metropolis"},{"text":"Data Analytics","href":"https:\/\/developer.nvidia.com\/machine-learning"},{"text":"Recommender Systems","href":"https:\/\/developer.nvidia.com\/nvidia-merlin"},{"text":"Computer Vision","href":"https:\/\/developer.nvidia.com\/computer-vision"},{"text":"Accelerate AI Applications","href":"https:\/\/developer.nvidia.com\/ai-for-accelerating-creative-applications"}]},{"name":"High-Performance Computing","links":[{"text":"Overview","href":"https:\/\/developer.nvidia.com\/hpc"},{"text":"Genomics","href":"https:\/\/developer.nvidia.com\/clara-genomics"},{"text":"Scientific Visualization","href":"https:\/\/developer.nvidia.com\/nvidia-index"},{"text":"Simulation \u0026 Modeling","href":"https:\/\/developer.nvidia.com\/hpc-sdk"},{"text":"Computational Lithography","href":"https:\/\/developer.nvidia.com\/culitho"}]},{"name":"Intelligent Machines","links":[{"text":"Overview","href":"https:\/\/developer.nvidia.com\/embedded-computing"},{"text":"Embedded and Edge AI","href":"https:\/\/developer.nvidia.com\/embedded-computing"},{"text":"Robotics","href":"https:\/\/developer.nvidia.com\/isaac"},{"text":"AI-Enabled Video Analytics","href":"https:\/\/developer.nvidia.com\/metropolis"},{"text":"Hardware (Jetson)","href":"https:\/\/developer.nvidia.com\/embedded\/jetson-modules"}]},{"name":"Rendering","links":[{"text":"Overview","href":"https:\/\/developer.nvidia.com\/rendering-technologies"},{"text":"Rendering Performance Tools","href":"https:\/\/developer.nvidia.com\/rendering-performance"},{"text":"Image Processing","href":"https:\/\/developer.nvidia.com\/image-processing"},{"text":"Graphics Research Tools","href":"https:\/\/developer.nvidia.com\/graphics-research-tools"},{"text":"Ray Tracing","href":"https:\/\/developer.nvidia.com\/rtx\/raytracing"}]},{"name":"Simulation","links":[{"text":"Physics and Dynamics Simulation","href":"https:\/\/developer.nvidia.com\/physx-sdk"},{"text":"Medical Imaging","href":"https:\/\/developer.nvidia.com\/clara"},{"text":"Scientific Visualization","href":"https:\/\/developer.nvidia.com\/nvidia-index"},{"text":"AR and VR Acceleration","href":"https:\/\/developer.nvidia.com\/vrworks"},{"text":"XR Streaming","href":"https:\/\/developer.nvidia.com\/nvidia-cloudxr-sdk"},{"text":"Robotics Simulation","href":"https:\/\/developer.nvidia.com\/isaac-sim"}]},{"name":"Game Engines","links":[{"text":"Overview","href":"https:\/\/developer.nvidia.com\/game-engines\/"},{"text":"Unreal Engine","href":"https:\/\/developer.nvidia.com\/game-engines\/unreal-engine"},{"text":"Unity","href":"https:\/\/developer.nvidia.com\/game-engines\/unity-engine"}]},{"name":"Networking","links":[{"text":"Overview","href":"https:\/\/developer.nvidia.com\/networking"},{"text":"DOCA","href":"https:\/\/developer.nvidia.com\/networking\/doca"},{"text":"HPC-X","href":"https:\/\/developer.nvidia.com\/networking\/hpc-x"},{"text":"Magnum IO","href":"https:\/\/developer.nvidia.com\/magnum-io"},{"text":"Rivermax","href":"https:\/\/developer.nvidia.com\/networking\/rivermax"}]},{"name":"Video, Broadcast and Display","links":[{"text":"Overview","href":"https:\/\/developer.nvidia.com\/video-and-audio-solutions"},{"text":"Display and Output","href":"https:\/\/www.nvidia.com\/en-us\/design-visualization\/solutions\/quadro-display-desktop-management#Management"},{"text":"Display and Output Solutions","href":"https:\/\/developer.nvidia.com\/display-and-output-solutions"},{"text":"HMD Support","href":"https:\/\/developer.nvidia.com\/vrworks"},{"text":"Motion Estimation","href":"https:\/\/developer.nvidia.com\/opticalflow-sdk"},{"text":"Latency Optimization","href":"https:\/\/developer.nvidia.com\/reflex"},{"text":"Virtual Collaboration \u0026 Content Creation","href":"https:\/\/developer.nvidia.com\/maxine"},{"text":"Video Decode and Encode","href":"https:\/\/developer.nvidia.com\/nvidia-video-codec-sdk"},{"text":"Video and Broadcast Networking","href":"https:\/\/www.mellanox.com\/solutions\/media-entertainment\/"},{"text":"AI-Enabled Video Analytics","href":"https:\/\/developer.nvidia.com\/metropolis"}]},{"name":"Autonomous Vehicles","links":[{"text":"Overview","href":"https:\/\/developer.nvidia.com\/drive"},{"text":"AV Development Platform","href":"https:\/\/developer.nvidia.com\/drive\/drive-hyperion"},{"text":"Modular Software Stack","href":"https:\/\/developer.nvidia.com\/drive\/drive-sdk"},{"text":"Simulation Platform","href":"https:\/\/developer.nvidia.com\/drive\/drive-sim"},{"text":"DNN Training Platform","href":"https:\/\/www.nvidia.com\/en-us\/data-center\/dgx-systems\/"}]},{"name":"Tools and Management","links":[{"text":"Arm Developer Tools","href":"https:\/\/developer.nvidia.com\/arm"},{"text":"Developer Tools","href":"https:\/\/developer.nvidia.com\/tools-overview"},{"text":"Android for Mobile","href":"https:\/\/developer.nvidia.com\/tools-overview"}]},{"name":"Telecommunications","links":[{"text":"Overview","href":"https:\/\/developer.nvidia.com\/industries\/telecommunications"},{"text":"CloudRAN","href":"https:\/\/developer.nvidia.com\/aerial-sdk"},{"text":"Sionna","href":"https:\/\/developer.nvidia.com\/sionna"}]},{"name":"GPU-Optimized Software","links":[{"text":"AI and HPC Containers","href":"https:\/\/developer.nvidia.com\/ai-hpc-containers"},{"text":"AI Models","href":"https:\/\/developer.nvidia.com\/ai-models"},{"text":"Jupyter Notebooks","href":"https:\/\/developer.nvidia.com\/jupyter-notebooks"},{"text":"NGC Catalog","href":"https:\/\/ngc.nvidia.com\/catalog"}]}]},{"title":"Platforms","sections":[{"name":"CUDA-X AI","links":[{"text":"TensorRT","href":"https:\/\/developer.nvidia.com\/tensorrt"},{"text":"Triton Inference Server","href":"https:\/\/developer.nvidia.com\/nvidia-triton-inference-server"},{"text":"NeMo","href":"https:\/\/developer.nvidia.com\/nvidia-nemo"},{"text":"cuDNN","href":"https:\/\/developer.nvidia.com\/cudnn"},{"text":"NCCL","href":"https:\/\/developer.nvidia.com\/nccl"},{"text":"DALI","href":"https:\/\/developer.nvidia.com\/dali"},{"text":"cuBLAS","href":"https:\/\/developer.nvidia.com\/cublas"},{"text":"cuSPARSE","href":"https:\/\/developer.nvidia.com\/cusparse"},{"text":"Optical Flow SDK","href":"https:\/\/developer.nvidia.com\/opticalflow-sdk"},{"text":"RAPIDS","href":"https:\/\/developer.nvidia.com\/rapids"}]},{"name":"DOCA","links":[{"text":"DOCA","href":"https:\/\/developer.nvidia.com\/networking\/doca"}]},{"name":"CLARA","links":[{"text":"Clara Guardian","href":"https:\/\/developer.nvidia.com\/clara-guardian"},{"text":"Clara Parabricks","href":"https:\/\/developer.nvidia.com\/clara-parabricks"}]},{"name":"HPC","links":[{"text":"HPC SDK","href":"https:\/\/developer.nvidia.com\/hpc-sdk"},{"text":"CUDA Toolkit","href":"https:\/\/developer.nvidia.com\/cuda-toolkit"},{"text":"OpenACC","href":"https:\/\/developer.nvidia.com\/openacc"},{"text":"IndeX","href":"https:\/\/developer.nvidia.com\/nvidia-index"},{"text":"CUDA-X Libraries","href":"https:\/\/developer.nvidia.com\/gpu-accelerated-libraries"},{"text":"Developer Tools","href":"https:\/\/developer.nvidia.com\/tools-overview"},{"text":"Modulus","href":"https:\/\/developer.nvidia.com\/modulus"},{"text":"cuLitho","href":"https:\/\/developer.nvidia.com\/culitho"}]},{"name":"Quantum Computing","links":[{"text":"CUDA Quantum","href":"https:\/\/developer.nvidia.com\/qoda"},{"text":"cuQuantum","href":"https:\/\/developer.nvidia.com\/cuquantum-sdk"}]},{"name":"DRIVE","links":[{"text":"DRIVE Hyperion","href":"https:\/\/developer.nvidia.com\/drive\/drive-hyperion"},{"text":"DRIVE SDK","href":"https:\/\/developer.nvidia.com\/drive\/drive-sdk"},{"text":"DRIVE OS","href":"https:\/\/developer.nvidia.com\/drive\/driveos"},{"text":"DriveWorks","href":"https:\/\/developer.nvidia.com\/drive\/driveworks"},{"text":"DRIVE AV","href":"https:\/\/developer.nvidia.com\/drive\/drive-sdk"},{"text":"DRIVE IX","href":"https:\/\/developer.nvidia.com\/drive\/drive-ix"},{"text":"DRIVE Sim","href":"https:\/\/developer.nvidia.com\/drive\/drive-sim"},{"text":"DRIVE Constellation","href":"https:\/\/developer.nvidia.com\/drive\/drive-constellation"}]},{"name":"ISAAC","links":[{"text":"Isaac SDK","href":"https:\/\/developer.nvidia.com\/isaac-sdk"},{"text":"Isaac Sim","href":"https:\/\/developer.nvidia.com\/isaac-sim"},{"text":"Jetson Developer Kits","href":"https:\/\/developer.nvidia.com\/embedded\/jetson-modules"},{"text":"Jetpack","href":"https:\/\/developer.nvidia.com\/embedded\/jetpack"}]},{"name":"RTX","links":[{"text":"DLSS","href":"https:\/\/developer.nvidia.com\/rtx\/dlss"},{"text":"Kickstart RT","href":"https:\/\/developer.nvidia.com\/rtx\/ray-tracing\/kickstart"},{"text":"Micro-Mesh","href":"https:\/\/developer.nvidia.com\/rtx\/ray-tracing\/micro-mesh"},{"text":"OptiX","href":"https:\/\/developer.nvidia.com\/rtx\/ray-tracing\/optix"},{"text":"RTX Direct Illumination (RTXDI)","href":"https:\/\/developer.nvidia.com\/rtx\/ray-tracing\/rtxdi"},{"text":"RTX Global Illumination (RTXGI)","href":"https:\/\/developer.nvidia.com\/rtx\/ray-tracing\/rtxgi"},{"text":"RTX Memory Utility (RTXMU)","href":"https:\/\/developer.nvidia.com\/rtx\/ray-tracing\/rtxmu"},{"text":"RTX Path Tracing","href":"https:\/\/developer.nvidia.com\/rtx\/path-tracing"},{"text":"Real-Time Denoisers (NRD)","href":"https:\/\/developer.nvidia.com\/rtx\/ray-tracing\/rt-denoisers"},{"text":"Reflex","href":"https:\/\/developer.nvidia.com\/rendering-performance\/reflex"},{"text":"Streamline","href":"https:\/\/developer.nvidia.com\/rtx\/streamline"}]},{"name":"Metropolis","links":[{"text":"Metropolis SDK","href":"https:\/\/developer.nvidia.com\/metropolis"},{"text":"DeepStream SDK","href":"https:\/\/developer.nvidia.com\/deepstream-sdk"},{"text":"TAO Toolkit","href":"https:\/\/developer.nvidia.com\/tao-toolkit"},{"text":"Metropolis Microservices","href":"https:\/\/developer.nvidia.com\/metropolis-microservices"},{"text":"Metropolis for Factories","href":"https:\/\/developer.nvidia.com\/metropolis-for-factories"}]},{"name":"Other Platforms","links":[{"text":"Aerial","href":"https:\/\/developer.nvidia.com\/aerial-sdk"},{"text":"Arm","href":"https:\/\/developer.nvidia.com\/arm"},{"text":"CloudXR","href":"https:\/\/developer.nvidia.com\/nvidia-cloudxr-sdk"},{"text":"DGX","href":"https:\/\/www.nvidia.com\/en-us\/data-center\/dgx-systems\/"},{"text":"DOCA","href":"https:\/\/developer.nvidia.com\/networking\/doca"},{"text":"Holoscan SDK","href":"https:\/\/developer.nvidia.com\/clara-holoscan-sdk"},{"text":"Riva","href":"https:\/\/developer.nvidia.com\/riva"},{"text":"Maxine","href":"https:\/\/developer.nvidia.com\/maxine"},{"text":"Merlin","href":"https:\/\/developer.nvidia.com\/nvidia-merlin"},{"text":"Omniverse","href":"https:\/\/developer.nvidia.com\/nvidia-omniverse-platform"},{"text":"cuOpt","href":"https:\/\/developer.nvidia.com\/cuopt-logistics-optimization"},{"text":"Rivermax","href":"https:\/\/developer.nvidia.com\/networking\/rivermax"},{"text":"TAO","href":"https:\/\/developer.nvidia.com\/TAO"},{"text":"Converged Accelerator","href":"https:\/\/developer.nvidia.com\/converged-accelerator-developer-kit"},{"text":"Morpheus","href":"https:\/\/developer.nvidia.com\/morpheus-cybersecurity"}]}]},{"title":"Industries","links":[{"text":"Financial Services","href":"https:\/\/developer.nvidia.com\/industries\/financial-services"},{"text":"Gaming","href":"https:\/\/developer.nvidia.com\/industries\/gamedev"},{"text":"Healthcare","href":"https:\/\/developer.nvidia.com\/healthcare-developer-resources"},{"text":"Higher Ed and Research","href":"https:\/\/developer.nvidia.com\/higher-education-and-research"},{"text":"Public Sector","href":"https:\/\/developer.nvidia.com\/industries\/public-sector"},{"text":"Transportation","href":"https:\/\/developer.nvidia.com\/transportation-developer-resources"},{"text":"Media and Entertainment","href":"https:\/\/developer.nvidia.com\/industries\/media-and-entertainment"},{"text":"See More","href":"https:\/\/developer.nvidia.com\/solutions-and-industries#industries"}]},{"title":"Resources","links":[{"text":"Contact Us","href":"https:\/\/developer.nvidia.com\/contact"},{"text":"Developer Program","href":"https:\/\/developer.nvidia.com\/developer-program"},{"text":"Training","href":"https:\/\/courses.nvidia.com\/"},{"text":"Educators","href":"https:\/\/developer.nvidia.com\/higher-education-and-research"},{"text":"NGC","href":"https:\/\/ngc.nvidia.com\/"},{"text":"NVIDIA GTC","href":"https:\/\/www.nvidia.com\/gtc\/"},{"text":"NVIDIA On-Demand","href":"https:\/\/www.nvidia.com\/en-us\/on-demand\/"},{"text":"Open Source","href":"https:\/\/developer.nvidia.com\/open-source"},{"text":"For Startups","href":"https:\/\/www.nvidia.com\/en-us\/deep-learning-ai\/startups\/"},{"text":"AI Playground","href":"https:\/\/www.nvidia.com\/en-us\/research\/ai-playground\/"}]}]}},"nvDeveloperFooter":{"locale":"en-US","menu":{"links":[{"text":"Legal Information","href":"https:\/\/www.nvidia.com\/en-us\/about-nvidia\/legal-info\/"},{"text":"Terms of Use","href":"https:\/\/developer.nvidia.com\/legal\/terms"},{"text":"Privacy Policy","href":"https:\/\/www.nvidia.com\/en-us\/about-nvidia\/privacy-policy\/"},{"text":"Cookie Policy","href":"https:\/\/www.nvidia.com\/en-us\/about-nvidia\/cookie-policy\/"},{"text":"Contact","href":"https:\/\/developer.nvidia.com\/contact"}]}},"bootstrap":{"anchorsFix":1,"anchorsSmoothScrolling":1,"formHasError":1,"popoverEnabled":1,"popoverOptions":{"animation":1,"html":0,"placement":"right","selector":"","trigger":"click","triggerAutoclose":1,"title":"","content":"","delay":0,"container":"body"},"tooltipEnabled":1,"tooltipOptions":{"animation":1,"html":0,"placement":"auto left","selector":"","trigger":"hover focus","delay":0,"container":"body"}}});
//--><!]]>
</script></head><body class="html not-front not-logged-in no-sidebars page-taxonomy page-taxonomy-term page-taxonomy-term- page-taxonomy-term-1438" ><div id="skip-link"> <a href="#main-content" class="element-invisible element-focusable">Skip to main content</a></div><div id="header"></div><script src="/sites/all/modules/custom/nvidia_components/js/header-component.js"></script><div id="wrapper"><div id="content-background" class="white-background"><div id='console'><div class="container"></div></div><div class="separator"></div><div id="content" class="container"><ol class="breadcrumb hidden-xs"><li><a href="/">Home</a></li><li><a href="/gpugems/gpugems3">GPUGems3</a></li><li class="active"><a href="/gpugems/gpugems3/part-iv-image-effects">Part IV: Image Effects</a></li></ol><div class="separator"></div><div class="row"> <section class="col-sm-12"> <a id="main-content"></a><div class="region region-content"> <section id="block-system-main" class="block block-system clearfix"><div class="term-listing-heading"><div id="book_switch"> <a href="/gpugems/gpugems" class="btn btn-primary">GPUGems</a><a href="/gpugems/gpugems2" class="btn btn-primary">GPUGems2</a><a href="/gpugems/gpugems3" class="btn btn-primary">GPUGems3</a></div><div id="book_page"><div class="row"><div class="col-md-8"> <a href="http://developer.nvidia.com/gpugems3"><img src="/sites/all/modules/custom/gpugems/books/GPUGems3/gpu_gems_3_icon.jpg" align="left" border="0" hspace="5"></a><h1><a href="http://developer.nvidia.com/gpugems3">GPU Gems 3</a></h1> <b>GPU Gems 3</b> is now available for free online!<br> <br> The CD content, including demos and content, is available on the <a target="_blank" href="https://http.download.nvidia.com/developer/GPU_Gems_3/CD/">web</a> and for <a href="https://http.download.nvidia.com/developer/GPU_Gems_3/CD/GPU_Gems_3_code.zip">download</a>.<br> <br> You can also subscribe to our <a href="http://news.developer.nvidia.com/rss.xml">Developer News Feed</a> to get notifications of new material on the site.<br> <br> <br><hr><h1 class="docChapterTitle" data-parent="Part IV: Image Effects">Chapter 24. The Importance of Being Linear</h1><p><em>Larry Gritz<br /> NVIDIA Corporation</em></p><p><em>Eugene d'Eon<br /> NVIDIA Corporation</em></p><h2>24.1 Introduction</h2><p>The performance and programmability of modern GPUs allow highly realistic lighting and shading to be achieved in real time. However, a subtle nonlinear property of almost every device that captures or displays digital images necessitates careful processing of textures and frame buffers to ensure that all this lighting and shading is computed and displayed correctly. Proper gamma correction is probably the easiest, most inexpensive, and most widely applicable technique for improving image quality in real-time applications.</p><h2>24.2 Light, Displays, and Color Spaces</h2><h4>24.2.1 Problems with Digital Image Capture, Creation, and Display</h4><p>If you are interested in high-quality rendering, you might wonder if the images displayed on your CRT, LCD, film, or paper will result in light patterns that are similar enough to the "real world" situation so that your eye will perceive them as realistic. You may be surprised to learn that there are several steps in the digital image creation pipeline where things can go awry. In particular:</p><ul><li>Does the capture of the light (by a camera, scanner, and so on) result in a digital image whose numerical values accurately represent the relative light levels that were present? If twice as many photons hit the sensor, will its numerical value be twice as high?</li><li>Does a producer of synthetic images, such as a renderer, create digital images whose pixel values are proportional to what light would really do in the situation they are simulating?</li><li>Does the display accurately turn the digital image back into light? If a pixel's numerical value is doubled, will the CRT, LCD, or other similar device display an image that appears twice as bright?</li></ul><p>The answer to these questions is, surprisingly, <em>probably not.</em> In particular, both the capture (scanning, painting, and digital photography) and the display (CRT, LCD, or other) are likely not linear processes, and this can lead to incorrect and unrealistic images if care is not taken at these two steps of the pipeline.</p><p>The nonlinearity is subtle enough that it is often unintentionally or intentionally ignored, particularly in real-time graphics. However, its effects on rendering, particularly in scenes with plenty of dynamic range like <a href="javascript:popUp('elementLinks/24fig01.jpg')">Figure 24-1</a>, are quite noticeable, and the simple steps needed to correct it are well worth the effort.</p><div class="figure"> <a href="javascript:popUp('elementLinks/24fig01.jpg')"><img src="/sites/all/modules/custom/gpugems/books/GPUGems3/elementLinks/24fig01.jpg" alt="24fig01.jpg" /></a><p><a href="javascript:popUp('elementLinks/24fig01.jpg')">Figure 24-1</a> The Benefit of Proper Gamma Correction</p></div><h4>24.2.2 Digression: What Is <em>Linear</em>?</h4><p>In the mathematical sense, a <em>linear</em> transformation is one in which the relationship between inputs and outputs is such that:</p><ul><li>The output of the sum of inputs is equal to the sum of the outputs of the individual inputs. That is, <em>f</em> (<em>x</em> + <em>y</em>) = <em>f</em> (<em>x</em>) + <em>f</em> (<em>y</em>).</li><li>The output scales by the same factor as a scale of the input (for a scalar <em>k</em>): <em>f</em>(<em>k</em> x <em>x</em>) = <em>k</em> x <em>f</em> (<em>x</em>).</li></ul><p>Light transport is linear. The illumination contributions from two light sources in a scene will <em>sum</em>. They will not multiply, subtract, or interfere with each other in unobvious ways. <sup><a href="javascript:popUp('elementLinks/ch24fn01.html')">[1]</a></sup></p><h4>24.2.3 Monitors Are Nonlinear, Renderers Are Linear</h4><p>CRTs do not behave linearly in their conversion of voltages into light intensities. And LCDs, although they do not inherently have this property, are usually constructed to mimic the response of CRTs.</p><p>A monitor's response is closer to an exponential curve, as shown in <a href="javascript:popUp('elementLinks/24fig02.jpg')">Figure 24-2</a>, and the exponent is called <em>gamma.</em> A typical gamma of 2.2 means that a pixel at 50 percent intensity emits less than a quarter of the light as a pixel at 100 percent intensity&#x2014;not half, as you would expect! Gamma is different for every individual display device, <sup><a href="javascript:popUp('elementLinks/ch24fn02.html')">[2]</a></sup> but typically it is in the range of 2.0 to 2.4. Adjusting for the effects of this nonlinear characteristic is called <em>gamma correction</em>.</p><div class="figure"> <a href="javascript:popUp('elementLinks/24fig02.jpg')"><img src="/sites/all/modules/custom/gpugems/books/GPUGems3/elementLinks/24fig02.jpg" alt="24fig02.jpg" /></a><p><a href="javascript:popUp('elementLinks/24fig02.jpg')">Figure 24-2</a> Typical Response Curve of a Monitor</p></div><p>Note that regardless of the exponent applied, the values of <em>black</em> (zero) and <em>white</em> (one) will always be the same. It's the intermediate, midtone values that will be corrected or distorted.</p><p>Renderers, shaders, and compositors like to operate with linear data. They <em>sum</em> the contributions of multiple light sources and multiply light values by reflectances (such as a constant value or a lookup from a texture that represents diffuse reflectivity).</p><p>But there are hidden assumptions&#x2014;such as, that a texture map that indicates how reflectivity varies over the surface also has a linear scale. Or that, upon display, the light intensities seen by the viewer will be indeed proportional to the values the renderer stored in the frame buffer.</p><p>The point is, if you have nonlinear inputs, then a renderer, shader, or compositor will "do the math wrong." This is an impediment to achieving realistic results.</p><p>Consider a digital painting. If you weren't viewing with "proper gamma correction" while you were creating the painting, the painting will contain "hidden" nonlinearities. The painting may look okay on <em>your</em> monitor, but it may appear different on other monitors and may not appear as you expected if it's used in a 3D rendering. The renderer will not actually use the values correctly if it assumes the image is linear. Also, if you take the implicitly linear output of a renderer and display it without gamma correction, the result will look too dark&#x2014;but not uniformly too dark, so it's not enough to just make it brighter.</p><p>If you err on both ends of the imaging pipeline&#x2014;you paint or capture images in gamma (nonlinear) space, render linear, and then fail to correct when displaying the rendered images&#x2014;the results <em>might</em> look okay at first glance. Why? Because the nonlinear painting/capture and the nonlinear display may cancel each other out to some degree. But there will be subtle artifacts riddled through the rendering and display process, including colors that change if a nonlinear input is brightened or darkened by lighting, and alpha-channel values that are wrong (compositing artifacts), and so your mipmaps were made wrong (texture artifacts). Plus, any attempts at realistic lighting&#x2014;such as high dynamic range (HDR) and imaged-based lighting&#x2014;are not really doing what you expect.</p><p>Also, your results will tend to look different for everybody who displays them because the paintings and the output have both, in some sense, a built-in reference to a particular monitor&#x2014;and not necessarily the same monitors will be used for creating the paintings and 3D lighting!</p><h2>24.3 The Symptoms</h2><p>If you ignore the problem&#x2014;paint or light in monitor space and display in monitor space&#x2014;you may encounter the following symptoms.</p><h4>24.3.1 Nonlinear Input Textures</h4><p>The average user doesn't have a calibrated monitor and has never heard of gamma correction; therefore, many visual materials are precorrected for them. For example, by convention, all JPEG files are precorrected for a gamma of 2.2. That's not exact for any monitor, but it's in the ballpark, so the image will probably look acceptable on most monitors. This means that JPEG images (including scans and photos taken with a digital camera) are not linear, so they should not be used as texture maps by shaders that assume linear input.</p><p>This precorrected format is convenient for directly displaying images on the average LCD or CRT display. And for storage of 8-bit images, it affords more "color resolution" in the darks, where the eye is more sensitive to small gradations of intensity. However, this format requires that these images be processed before they are used in any kind of rendering or compositing operation.</p><h4>24.3.2 Mipmaps</h4><p>When creating a mipmap, the obvious way to downsize at each level is for each lower-mip texel to be one-fourth the sum of the four texels "above" it at the higher resolution. Suppose at the high-resolution level you have an edge: two pixels at 1.0 and two at 0.0. The low-resolution pixel ought to be 0.5, right? Half as bright, because it's half fully bright and half fully dark? And that's surely how you're computing the mipmap. Aha, but that's <em>linear math.</em> If you are in a nonlinear color space with a gamma, say of 2.0, then that coarse-level texel with a value of 0.5 will be displayed at only 25 percent of the brightness. So you will see not-so-subtle errors in low-resolution mipmap levels, where the intensities simply aren't the correctly filtered results of the high-resolution levels. The brightness of rendered objects will change along contour edges and with distance from the 3D camera.</p><p>Furthermore, there is a subtle problem with texture-filtering nonlinear textures on the GPU (or even CPU renderers), even if you've tried to correct for nonlinearities when creating the mipmaps. The GPU uses the texture lookup filter extents to choose a texture level of detail (LOD), or "mipmap level," and will blend between two adjacent mipmap levels if necessary. We hope by now you've gotten the gist of our argument and understand that if the render-time blend itself is also done assuming linear math&#x2014;and if the two texture inputs are nonlinear&#x2014;the results won't quite give the right color intensities. This situation will lead to textures subtly pulsating in intensity during transitions between mipmap levels.</p><h4>24.3.3 Illumination</h4><p>Consider the illumination of a simple Lambertian sphere, as shown in <a href="javascript:popUp('elementLinks/24fig03.jpg')">Figure 24-3</a>. <sup><a href="javascript:popUp('elementLinks/ch24fn03.html')">[3]</a></sup> If the reflected light is proportional to <strong>N</strong> &#xB7; <strong>L</strong>, then a spot <em>A</em> on the sphere where <strong>N</strong> and <strong>L</strong> form a 60-degree angle should reflect half the light toward the camera. Thus, it appears half as bright as a spot <em>B</em> on the sphere, where <strong>N</strong> points straight to the light source. If these image values were converted to light linearly, when you view the image it should look like a real object with those reflective properties. But if you display the image on a monitor with a gamma of 2.0, <em>A</em> will actually only be 0.5 <em><sup>gamma</sup></em> , or one-fourth as bright as <em>B</em>.</p><div class="figure"> <a href="javascript:popUp('elementLinks/24fig03.jpg')"><img src="/sites/all/modules/custom/gpugems/books/GPUGems3/elementLinks/24fig03.jpg" alt="24fig03.jpg" /></a><p><a href="javascript:popUp('elementLinks/24fig03.jpg')">Figure 24-3</a> A Linear Image Gamma-Corrected () and Uncorrected ()</p></div><p>In other words, computer-generated (CG) materials and lights will simply not match the appearance of real materials and lights if you render assuming linear properties of light but display on a nonlinear monitor. Overall, the scene will appear dimmer than an accurate simulation. However, merely brightening it by a constant factor is not good enough: The brights will be more correct than the midrange intensities. Shadow terminators and intensity transitions will be sharper&#x2014;for example, the transition from light to dark will be faster&#x2014;than in the real world. Corners will look too dark. And the more "advanced" lighting techniques that you use (such as HDR, global illumination of any kind, and subsurface scattering), the more critical it will become to stick to a linear color space to match the linear calculations of your sophisticated lighting.</p><h4>24.3.4 Two Wrongs Don't Make a Right</h4><p>The most common gamma mistake made in rendering is using nonlinear color textures for shading and then not applying gamma correction to the final image. This double error is much less noticeable than making just one of these mistakes, because the corrections required for each are roughly opposites. However, this situation creates many problems that can be easily avoided.</p><p><a href="javascript:popUp('elementLinks/24fig04.jpg')">Figure 24-4</a> shows a comparison between two real-time renderings using a realistic skin shader. (For more information on skin shaders, see Chapter 14 of this book, "Advanced Techniques for Realistic Real-Time Skin Rendering.") The left image converted the diffuse color texture into a linear space because the texture came from several JPEG photographs. Lighting (including subsurface scattering) and shading were performed correctly in a linear space and then the final image was gamma-corrected for display on average nonlinear display devices.</p><div class="figure"> <a href="javascript:popUp('elementLinks/24fig04.jpg')"><img src="/sites/all/modules/custom/gpugems/books/GPUGems3/elementLinks/24fig04.jpg" alt="24fig04.jpg" /></a><p><a href="javascript:popUp('elementLinks/24fig04.jpg')">Figure 24-4</a> Rendering with Proper Gamma Correction () and Rendering Ignoring Gamma ()</p></div><p>The image on the right made neither of these corrections and exhibits several problems. The skin tone from the color photographs is changed because the lighting was performed in a nonlinear space, and as the light brightened and darkened the color values, the colors were inadvertently changed. (The red levels of the skin tones are higher than the green and blue levels and thus receive a different boost when brightened or darkened by light and shadow.) The white specular light, when added to the diffuse lighting, becomes yellow. The shadowed regions become too dark, and the subsurface scattering (particularly the subtle red diffusion into shadowed regions) is almost totally missing because it is squashed by the gamma curve when it's displayed.</p><p>Adjusting lighting becomes problematic in a nonlinear space. If we take the same scenes in <a href="javascript:popUp('elementLinks/24fig04.jpg')">Figure 24-4</a> and render them now with the light intensities increased, the problems in the nonlinear version become worse, as you can see in <a href="javascript:popUp('elementLinks/24fig05.jpg')">Figure 24-5</a>. As a rule, if color tones change as the same object is brightened or darkened, then most likely nonlinear lighting is taking place and gamma correction is needed.</p><div class="figure"> <a href="javascript:popUp('elementLinks/24fig05.jpg')"><img src="/sites/all/modules/custom/gpugems/books/GPUGems3/elementLinks/24fig05.jpg" alt="24fig05.jpg" /></a><p><a href="javascript:popUp('elementLinks/24fig05.jpg')">Figure 24-5</a> Rendering with Proper Gamma Correction () and Rendering Incorrect Gamma ()</p></div><p>A common problem encountered when subsurface scattering is used for skin rendering (when gamma correction is ignored) is the appearance of a blue-green glow around the shadow edges and an overly waxy-looking skin, as shown in <a href="javascript:popUp('elementLinks/24fig06.jpg')">Figure 24-6</a>. These problems arise when the scattering parameters are adjusted to give shadow edges the desired amount of red bleeding (as seen in <a href="javascript:popUp('elementLinks/24fig05.jpg')">Figure 24-5</a>). This effect is hard to achieve in a nonlinear color space and requires a heavily weighted broad blur of irradiance in the red channel. The result causes far too much diffusion in the bright regions of the face (giving a waxy look) and causes the red to darken too much as it approaches a shadow edge (leaving a blue-green glow).</p><div class="figure"> <a href="javascript:popUp('elementLinks/24fig06.jpg')"><img src="/sites/all/modules/custom/gpugems/books/GPUGems3/elementLinks/24fig06.jpg" alt="24fig06.jpg" /></a><p><a href="javascript:popUp('elementLinks/24fig06.jpg')">Figure 24-6</a> Tweaking Subsurface Scattering When Rendering Without Gamma Correction Is Problematic</p></div><h2>24.4 The Cure</h2><p>Gamma correction is the practice of applying the <em>inverse</em> of the monitor transformation to the image pixels before they're displayed. That is, if we raise pixel values to the power 1/<em>gamma</em> before display, then the display implicitly raising to the power <em>gamma</em> will exactly cancel it out, resulting, overall, in a linear response (see <a href="javascript:popUp('elementLinks/24fig02.jpg')">Figure 24-2</a>).</p><p>The usual implementation is to have your windowing systems apply color-correction <em>lookup tables</em> (LUTs) that incorporate gamma correction. This adjustment allows all rendering, compositing, or other image manipulation to use linear math and for any linear-scanned images, paintings, or rendered images to be displayed correctly.</p><p>Animation and visual effects studios, as well as commercial publishers, are very careful with this process. Often it's the exclusive job of a staff person to understand color management for their monitors, compositing pipeline, film and video scanning, and final outputs. In fact, for high-end applications, simple gamma correction is not enough. Often compositors use a much more sophisticated 3D color LUT derived from careful measurements of the individual displays or film stocks' color-response curves. <sup><a href="javascript:popUp('elementLinks/ch24fn04.html')">[4]</a></sup> In contrast to visual effects and animation for film, game developers often get this process wrong, which leads to the artifacts discussed in this chapter. This is one reason why most (but not all) CG for film looks much better than games&#x2014;a reason that has nothing to do with the polygon counts, shading, or artistic skills of game creators. (It's also sometimes a reason why otherwise well-made film CG looks poor&#x2014;because the color palettes and gammas have been mismatched by a careless compositor.)</p><p>Joe GamePlayer doesn't have a calibrated monitor. We don't know the right LUTs to apply for his display, and in any case, he doesn't want to apply gamma-correcting LUTs to his entire display. Why? Because then the ordinary JPEG files viewed with Web browsers and the like will look washed out and he won't understand why. (Film studios don't care if random images on the Internet look wrong on an artist's workstation, as long as their actual work product&#x2014;the film itself&#x2014;looks perfect in the theatre, where they have complete control.) But simple gamma correction for the "average" monitor can get us most of the way there. The remainder of this chapter will present the easiest solutions to improve and avoid these artifacts in games you are developing.</p><h4>24.4.1 Input Images (Scans, Paintings, and Digital Photos)</h4><p>Most images you capture with scanning or digital photography are likely already gamma-corrected (especially if they are JPEGs by the time they get to you), and therefore are in a nonlinear color space. (Digital camera JPEGs are usually sharpened by the camera as well for capturing textures; try to use a RAW file format to avoid being surprised.) If you painted textures without using a gamma LUT, those paintings will also be in monitor color space. If an image looks right on your monitor in a Web browser, chances are it has been gamma-corrected and is nonlinear.</p><p>Any input textures that are already gamma-corrected need to be brought back to a linear color space before they can be used for shading or compositing. You want to make this adjustment for texture values that are used for (linear) lighting calculations. Color values (such as light intensities and diffuse reflectance colors) should always be uncorrected to a linear space before they're used in shading. Alpha channels, normal maps, displacement values (and so on) are almost certainly already linear and should not be corrected further, nor should any textures that you were careful to paint or capture in a linear fashion.</p><p>All modern GPUs support sRGB texture formats. These formats allow binding gamma-corrected pixel data directly as a texture with proper gamma applied by the hardware before the results of texture fetches are used for shading. On NVIDIA GeForce 8-class (and future) hardware, all samples used in a texture filtering operation are linearized before filtering to ensure the proper filtering is performed (older GPUs apply the gamma post-filtering). The correction applied is an IEC standard (IEC 61966-2-1) that corresponds to a gamma of roughly 2.2 and is a safe choice for nonlinear color data where the exact gamma curve is not known. Alpha values, if present, are not corrected.</p><p>Appropriate sRGB formats are defined for all 8-bit texture formats (RGB, RGBA, luminance, luminance alpha, and DXT compressed), both in OpenGL and in DirectX. Passing <tt>GL_SRGB_EXT</tt> instead of <tt>GL_RGB</tt> to <tt>glTexImage2D</tt>, for example, ensures that any shader accesses to the specified texture return linear pixel values.</p><p>The automatic sRGB corrections are free and are preferred to performing the corrections manually in a shader after each texture access, as shown in Listing 24-1, because each <tt>pow</tt> instruction is scalar and expanded to two instructions. Also, manual correction happens after filtering, which is incorrectly performed in a nonlinear space. The sRGB formats may also be preferred to preconverting textures to linear versions before they are loaded. Storing linear pixels back into an 8-bit image is effectively a loss of precision in low light levels and can cause banding when the pixels are converted back to monitor space or used in further shading.</p><h4>Example 24-1. Manually Converting Color Values to a Linear Space</h4><p><em>Texture lookups can apply inverse gamma correction so that the rest of your shader is working with linear values. However, using an sRGB texture is faster, allows proper linear texture filtering (GeForce 8 and later), and requires no extra shader code.</em></p><pre name="code" class="cpp:nocontrols">    float3 diffuseCol = pow( f3tex2D( diffTex, texCoord ), 2.2 ); // Or (cheaper, but assuming gamma of 2.0 rather than 2.2)    float3 diffuseCol = f3tex2D( diffTex, texCoord ); diffuseCol = diffuseCol * diffuseCol; </pre><p>Managing shaders that need to mix linear and nonlinear inputs can be an ugly logistical chore for the engine programmers and artists who provide textures. The simplest solution, in many cases, is to simply require all textures to be precorrected to linear space before they're delivered for use in rendering.</p><h4>24.4.2 Output Images (Final Renders)</h4><p>The <em>last</em> step before display is to gamma-correct the final pixel values so that when they're displayed on a monitor with nonlinear response, the image looks "correct." Specifying an sRGB frame buffer leaves the correction to the GPU, and no changes to shaders are required. Any value returned in the shader is gamma-corrected before storage in the frame buffer (or render-to-texture buffer). Furthermore, on GeForce 8-class and later hardware, if blending is enabled, the previously stored value is converted back to linear before blending and the result of the blend is gamma-corrected. Alpha values are not gamma-corrected when sRGB buffers are enabled. If sRGB buffers are not available, you can use the more costly solution of custom shader code, as shown in Listing 24-2; however, any blending, if enabled, will be computed incorrectly.</p><h4>Example 24-2. Last-Stage-Output Gamma Correction</h4><p><em>If sRGB frame buffers are not available (or if a user-defined gamma value is exposed), the following code will perform gamma correction.</em></p><pre name="code" class="cpp:nocontrols">    float3 finalCol = do_all_lighting_and_shading(); float pixelAlpha = compute_pixel_alpha(); return float4(pow(finalCol, 1.0 / 2.2), pixelAlpha); // Or (cheaper, but assuming gamma of 2.0 rather than 2.2)    return float4( sqrt( finalCol ), pixelAlpha ); </pre><h4>24.4.3 Intermediate Color Buffers</h4><p>A few subtle points should be kept in mind. If you are doing any kind of post-processing pass on your images, you should be doing the gamma correction as the <em>last step</em> of the last post-processing pass. Don't render, correct, and then do further math on the result as if it were a linear image.</p><p>Also, if you are rendering to create a texture, you need to either (a) gamma-correct, and then treat the texture as a nonlinear input when performing any further processing of it, or (b) not gamma-correct, and treat the texture as linear input for any further processing. Intermediate color buffers may lose precision in the darks if stored as 8-bit linear images, compared to the precision they would have as gamma-corrected images. Thus, it may be beneficial to use 16-bit floating-point or sRGB frame-buffer and sRGB texture formats for rendering and accessing intermediate color buffers.</p><h2>24.5 Conclusion</h2><p>OpenGL, DirectX, and any shaders you write are probably performing math as if all texture inputs, light/material interactions, and outputs are <em>linear</em> (that is, light intensities sum; diffuse reflectivities multiply). But it is very likely that your texture inputs may be nonlinear, and it's almost certain that your user's uncalibrated and uncorrected monitor applies a nonlinear color-space transformation. This scenario leads to all sorts of artifacts and inaccuracies, some subtle (such as mipmap filtering errors) and some grossly wrong (such as very incorrect light falloff).</p><p>We strongly suggest that developers take the following simple steps:</p><ol><li>Assume that most game players are using uncalibrated, uncorrected monitors that can roughly be characterized by an exponential response with gamma = 2.2. (For an even higher-quality end-user experience: Have your game setup display a gamma calibration chart and let the user choose a good gamma value.)</li><li>When performing lookups from nonlinear textures (those that look "right" on uncorrected monitors) that represent light or color values, raise the results to the power <em>gamma</em> to yield a linear value that can be used by your shader. Do <em>not</em> make this adjustment for values already in a linear color space, such as certain high-dynamic-range light maps or images containing normals, bump heights, or other noncolor data. Use sRGB texture formats, if possible, for increased performance and correct texture filtering on GeForce 8 GPUs (and later).</li><li>Apply a gamma correction (that is, raise to power 1/<em>gamma</em>) to the final pixel values as <em>the very last step</em> before displaying them. Use sRGB frame-buffer extensions for efficient automatic gamma correction with proper blending.</li></ol><p>Carefully following these steps is crucial to improving the look of your game and especially to increasing the accuracy of any lighting or material calculations you are performing in your shaders.</p><h2>24.6 Further Reading</h2><p>In this chapter, we've tried to keep the descriptions and advice simple, with the intent of making a fairly nontechnical argument for why you should use linear color spaces. In the process, we've oversimplified. For those of you who crave the gory details, an excellent treatment of the gamma problem can be found on Charles Poynton's Web page: <a onclick="newwindow(this)" href="http://www.poynton.com/GammaFAQ.html">http://www.poynton.com/GammaFAQ.html</a>.</p><p>The Wikipedia entry on gamma correction is surprisingly good:</p><ul><li> <a onclick="newwindow(this)" href="http://en.wikipedia.org/wiki/Gamma_correction">http://en.wikipedia.org/wiki/Gamma_correction</a>.</li></ul><p>For details regarding sRGB hardware formats in OpenGL and DirectX, see these resources:</p><ul><li><a onclick="newwindow(this)" href="http://www.nvidia.com/dev_content/nvopenglspecs/GL_EXT_texture_sRGB.txt">http://www.nvidia.com/dev_content/nvopenglspecs/GL_EXT_texture_sRGB.txt</a></li><li><a onclick="newwindow(this)" href="http://www.opengl.org/registry/specs/EXT/framebuffer_sRGB.txt">http://www.opengl.org/registry/specs/EXT/framebuffer_sRGB.txt</a></li><li><a onclick="newwindow(this)" href="http://msdn2.microsoft.com/en-us/library/bb173460.aspx">http://msdn2.microsoft.com/en-us/library/bb173460.aspx</a></li></ul><p><em>Thanks to Gary King and Mark Kilgard for their expertise on sRGB and their helpful comments regarding this chapter. Special thanks to actor Doug Jones for kindly allowing us to use his likeness. And finally, <a href="javascript:popUp('elementLinks/24fig02.jpg')">Figure 24-2</a> was highly inspired by a very similar version we found on Wikipedia.</em></p></div><div class="col-md-4"><ul><li><a href="/gpugems/gpugems3/contributors" class="">Contributors</a></li><li><a href="/gpugems/gpugems3/foreword" class="">Foreword</a></li><li><a href="/gpugems/gpugems3/part-i-geometry" class="">Part I: Geometry</a><ul><li><a href="/gpugems/gpugems3/part-i-geometry/chapter-1-generating-complex-procedural-terrains-using-gpu" class="">Chapter 1. Generating Complex Procedural Terrains Using the GPU</a></li><li><a href="/gpugems/gpugems3/part-i-geometry/chapter-2-animated-crowd-rendering" class="">Chapter 2. Animated Crowd Rendering</a></li><li><a href="/gpugems/gpugems3/part-i-geometry/chapter-3-directx-10-blend-shapes-breaking-limits" class="">Chapter 3. DirectX 10 Blend Shapes: Breaking the Limits</a></li><li><a href="/gpugems/gpugems3/part-i-geometry/chapter-4-next-generation-speedtree-rendering" class="">Chapter 4. Next-Generation SpeedTree Rendering</a></li><li><a href="/gpugems/gpugems3/part-i-geometry/chapter-5-generic-adaptive-mesh-refinement" class="">Chapter 5. Generic Adaptive Mesh Refinement</a></li><li><a href="/gpugems/gpugems3/part-i-geometry/chapter-6-gpu-generated-procedural-wind-animations-trees" class="">Chapter 6. GPU-Generated Procedural Wind Animations for Trees</a></li><li><a href="/gpugems/gpugems3/part-i-geometry/chapter-7-point-based-visualization-metaballs-gpu" class="">Chapter 7. Point-Based Visualization of Metaballs on a GPU</a></li></ul></li><li><a href="/gpugems/gpugems3/part-ii-light-and-shadows" class="">Part II: Light and Shadows</a><ul><li><a href="/gpugems/gpugems3/part-ii-light-and-shadows/chapter-10-parallel-split-shadow-maps-programmable-gpus" class="">Chapter 10. Parallel-Split Shadow Maps on Programmable GPUs</a></li><li><a href="/gpugems/gpugems3/part-ii-light-and-shadows/chapter-11-efficient-and-robust-shadow-volumes-using" class="">Chapter 11. Efficient and Robust Shadow Volumes Using Hierarchical Occlusion Culling and Geometry Shaders</a></li><li><a href="/gpugems/gpugems3/part-ii-light-and-shadows/chapter-12-high-quality-ambient-occlusion" class="">Chapter 12. High-Quality Ambient Occlusion</a></li><li><a href="/gpugems/gpugems3/part-ii-light-and-shadows/chapter-13-volumetric-light-scattering-post-process" class="">Chapter 13. Volumetric Light Scattering as a Post-Process</a></li><li><a href="/gpugems/gpugems3/part-ii-light-and-shadows/chapter-8-summed-area-variance-shadow-maps" class="">Chapter 8. Summed-Area Variance Shadow Maps</a></li><li><a href="/gpugems/gpugems3/part-ii-light-and-shadows/chapter-9-interactive-cinematic-relighting-global" class="">Chapter 9. Interactive Cinematic Relighting with Global Illumination</a></li></ul></li><li><a href="/gpugems/gpugems3/part-iii-rendering" class="">Part III: Rendering</a><ul><li><a href="/gpugems/gpugems3/part-iii-rendering/chapter-14-advanced-techniques-realistic-real-time-skin" class="">Chapter 14. Advanced Techniques for Realistic Real-Time Skin Rendering</a></li><li><a href="/gpugems/gpugems3/part-iii-rendering/chapter-15-playable-universal-capture" class="">Chapter 15. Playable Universal Capture</a></li><li><a href="/gpugems/gpugems3/part-iii-rendering/chapter-16-vegetation-procedural-animation-and-shading-crysis" class="">Chapter 16. Vegetation Procedural Animation and Shading in Crysis</a></li><li><a href="/gpugems/gpugems3/part-iii-rendering/chapter-17-robust-multiple-specular-reflections-and-refractions" class="">Chapter 17. Robust Multiple Specular Reflections and Refractions</a></li><li><a href="/gpugems/gpugems3/part-iii-rendering/chapter-18-relaxed-cone-stepping-relief-mapping" class="">Chapter 18. Relaxed Cone Stepping for Relief Mapping</a></li><li><a href="/gpugems/gpugems3/part-iii-rendering/chapter-19-deferred-shading-tabula-rasa" class="">Chapter 19. Deferred Shading in Tabula Rasa</a></li><li><a href="/gpugems/gpugems3/part-iii-rendering/chapter-20-gpu-based-importance-sampling" class="">Chapter 20. GPU-Based Importance Sampling</a></li></ul></li><li><a href="/gpugems/gpugems3/part-iv-image-effects" class="active">Part IV: Image Effects</a><ul><li><a href="/gpugems/gpugems3/part-iv-image-effects/chapter-21-true-impostors" class="">Chapter 21. True Impostors</a></li><li><a href="/gpugems/gpugems3/part-iv-image-effects/chapter-22-baking-normal-maps-gpu" class="">Chapter 22. Baking Normal Maps on the GPU</a></li><li><a href="/gpugems/gpugems3/part-iv-image-effects/chapter-23-high-speed-screen-particles" class="">Chapter 23. High-Speed, Off-Screen Particles</a></li><li><a href="/gpugems/gpugems3/part-iv-image-effects/chapter-24-importance-being-linear" class="active">Chapter 24. The Importance of Being Linear</a></li><li><a href="/gpugems/gpugems3/part-iv-image-effects/chapter-25-rendering-vector-art-gpu" class="">Chapter 25. Rendering Vector Art on the GPU</a></li><li><a href="/gpugems/gpugems3/part-iv-image-effects/chapter-26-object-detection-color-using-gpu-real-time-video" class="">Chapter 26. Object Detection by Color: Using the GPU for Real-Time Video Image Processing</a></li><li><a href="/gpugems/gpugems3/part-iv-image-effects/chapter-27-motion-blur-post-processing-effect" class="">Chapter 27. Motion Blur as a Post-Processing Effect</a></li><li><a href="/gpugems/gpugems3/part-iv-image-effects/chapter-28-practical-post-process-depth-field" class="">Chapter 28. Practical Post-Process Depth of Field</a></li></ul></li><li><a href="/gpugems/gpugems3/part-v-physics-simulation" class="">Part V: Physics Simulation</a><ul><li><a href="/gpugems/gpugems3/part-v-physics-simulation/chapter-29-real-time-rigid-body-simulation-gpus" class="">Chapter 29. Real-Time Rigid Body Simulation on GPUs</a></li><li><a href="/gpugems/gpugems3/part-v-physics-simulation/chapter-30-real-time-simulation-and-rendering-3d-fluids" class="">Chapter 30. Real-Time Simulation and Rendering of 3D Fluids</a></li><li><a href="/gpugems/gpugems3/part-v-physics-simulation/chapter-31-fast-n-body-simulation-cuda" class="">Chapter 31. Fast N-Body Simulation with CUDA</a></li><li><a href="/gpugems/gpugems3/part-v-physics-simulation/chapter-32-broad-phase-collision-detection-cuda" class="">Chapter 32. Broad-Phase Collision Detection with CUDA</a></li><li><a href="/gpugems/gpugems3/part-v-physics-simulation/chapter-33-lcp-algorithms-collision-detection-using-cuda" class="">Chapter 33. LCP Algorithms for Collision Detection Using CUDA</a></li><li><a href="/gpugems/gpugems3/part-v-physics-simulation/chapter-34-signed-distance-fields-using-single-pass-gpu" class="">Chapter 34. Signed Distance Fields Using Single-Pass GPU Scan Conversion of Tetrahedra</a></li><li><a href="/gpugems/gpugems3/part-v-physics-simulation/chapter-35-fast-virus-signature-matching-gpu" class="">Chapter 35. Fast Virus Signature Matching on the GPU</a></li></ul></li><li><a href="/gpugems/gpugems3/part-vi-gpu-computing" class="">Part VI: GPU Computing</a><ul><li><a href="/gpugems/gpugems3/part-vi-gpu-computing/chapter-36-aes-encryption-and-decryption-gpu" class="">Chapter 36. AES Encryption and Decryption on the GPU</a></li><li><a href="/gpugems/gpugems3/part-vi-gpu-computing/chapter-37-efficient-random-number-generation-and-application" class="">Chapter 37. Efficient Random Number Generation and Application Using CUDA</a></li><li><a href="/gpugems/gpugems3/part-vi-gpu-computing/chapter-38-imaging-earths-subsurface-using-cuda" class="">Chapter 38. Imaging Earth&#039;s Subsurface Using CUDA</a></li><li><a href="/gpugems/gpugems3/part-vi-gpu-computing/chapter-39-parallel-prefix-sum-scan-cuda" class="">Chapter 39. Parallel Prefix Sum (Scan) with CUDA</a></li><li><a href="/gpugems/gpugems3/part-vi-gpu-computing/chapter-40-incremental-computation-gaussian" class="">Chapter 40. Incremental Computation of the Gaussian</a></li><li><a href="/gpugems/gpugems3/part-vi-gpu-computing/chapter-41-using-geometry-shader-compact-and-variable-length" class="">Chapter 41. Using the Geometry Shader for Compact and Variable-Length GPU Feedback</a></li></ul></li><li><a href="/gpugems/gpugems3/preface" class="">Preface</a></li></ul></div></div></div></div></section></div> </section></div></div><div class="separator"></div></div> <footer><div class="footer-links"><div class="container"><div class="row"><div class="col-xs-12 col-sm-12 col-md-3 col-lg-3"><div class="col-xs-12 col-sm-12 col-md-12 col-lg-12"><div class="padding-md-footer"><div class="logo-footer"></div></div></div><div class="col-xs-12 col-sm-12 col-md-9 col-lg-9 padding-section-footer"><div class="region region-footer-menu"><div class="block block-menu" id="block-menu-menu-footer-menu"><div class="block-content zone-select"><ul class="menu nav"><li class="first leaf nav-item"><a href="/hpc" title="" class="nav-link">HIGH PERFORMANCE COMPUTING</a></li><li class="leaf nav-item"><a href="/embedded-computing" title="" class="nav-link">JETPACK</a></li><li class="last leaf nav-item"><a href="/drive" title="" class="nav-link">DRIVE</a></li></ul></div></div></div></div></div><div class="col-xs-12 col-sm-12 col-md-9 col-lg-9"></div><div class="col-xs-12 col-sm-12 col-md-12 col-lg-12"></div></div></div></div><div class="footer-boilerplate"><div id="footer"></div><script src="/sites/all/modules/custom/nvidia_components/js/footer-component.js"></script></div> </footer></div><script type="text/javascript">
<!--//--><![CDATA[//><!--
_satellite.pageBottom();
//--><!]]>
</script><script type="text/javascript" src="/sites/default/files/advagg_js/js__vHPXA6YnKwonywmKr3fXKzYpiPwSOdlWbZ-Liup6d9I__QzJ-47mpM3whhrZXPx0zdKZcX6SbLbeNXBC-AY-D_7U__RdADzPRPOCdjuQ5t3WiEWu4q_r6oFSMi1EfqzG9BsYY.js"></script><script type="text/javascript">window.NREUM||(NREUM={});NREUM.info={"beacon":"bam.nr-data.net","licenseKey":"6f2048d7bc","applicationID":"341156206","transactionName":"YFVbbEJQXhJTW0JRX1kfeFtEWF8PHUxXQF9ZX1RBb1VZEkJUV0FvQ1FBV15eXRhtTFNKXWhAWF9V","queueTime":0,"applicationTime":323,"atts":"TBJYGgpKTRw=","errorBeacon":"bam.nr-data.net","agent":""}</script></body></html>